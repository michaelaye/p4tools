"""This module takes care of receiving the data catalogs."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/00_io.ipynb.

# %% auto 0
__all__ = ['logger', 'base_url', 'urls', 'hashes', 'fetch_zipped_file', 'get_blotch_catalog', 'get_fan_catalog', 'get_meta_data',
           'get_tile_coords', 'get_region_names', 'get_tile_urls', 'normalize_tile_id', 'get_subframe',
           'get_url_for_tile_id', 'get_url_for_tile', 'get_subframe_by_tile_id', 'get_subframe_for_tile',
           'get_fans_for_tile', 'get_blotches_for_tile', 'get_hirise_id_for_tile']

# %% ../notebooks/00_io.ipynb 2
import matplotlib.image as mplimg
import pandas as pd
import pooch
from matplotlib import pyplot as plt
from yarl import URL

# %% ../notebooks/00_io.ipynb 3
logger = pooch.get_logger()
logger.setLevel("WARNING")

# %% ../notebooks/00_io.ipynb 4
base_url = URL("https://zenodo.org/record/8102805/files/")
urls = {
    "fans": "P4_catalog_v1.1_L1C_cut_0.5_fan.csv.zip",
    "blotches": "P4_catalog_v1.1_L1C_cut_0.5_blotch.csv.zip",
    "metadata": "P4_catalog_v1.1_metadata.csv.zip",
    "tile_coords": "P4_catalog_v1.1_tile_coords_final.csv.zip",
    "raw_data": "P4_catalog_v1.0_raw_classifications.hdf.zip",
    "intermediate": "P4_catalog_v1.0_pipeline_products.zip",
    "region_names": "region_names.zip",
    "tile_urls": "tile_urls.csv.zip",
}

# %% ../notebooks/00_io.ipynb 5
hashes = {
    "fans": "md5:71ff51ff79d6e975f704f19b1996d8ea",
    "blotches": "md5:f4d0c101f65abbaf34e092620133d56e",
    "metadata": "md5:c0dc46e0fc3d259c30afaec412074eae",
    "tile_coords": "md5:6b9a917a6997f1aa01cfef4322cabd81",
    "raw_data": "md5:39a8909590fe9f816454db93f0027d2c",
    "intermediate": "md5:6544bf0c7851eedd4783859c0adc42d7",
    "region_names": "md5:9101c7a0f8e248c9ffe9c07869da5635",
    "tile_urls": "md5:5717c8379d453cf4b11a5f5775f5fb6e",
}

# %% ../notebooks/00_io.ipynb 6
def _get_hash(key):
    url = URL(urls[key])
    path = pooch.retrieve(str(url), progressbar=True, known_hash=None)
    return pooch.file_hash(path)


def fetch_zipped_file(key):
    url = base_url / urls[key]
    hash = hashes[key]
    fpath = pooch.retrieve(
        str(url),
        path=pooch.os_cache("p4tools"),
        known_hash=hash,
        processor=pooch.Unzip(),
        progressbar=True,
    )
    return fpath[0]

# %% ../notebooks/00_io.ipynb 8
def get_blotch_catalog() -> pd.DataFrame:
    return pd.read_csv(fetch_zipped_file("blotches"))


def get_fan_catalog() -> pd.DataFrame:
    return pd.read_csv(fetch_zipped_file("fans"))


def get_meta_data() -> pd.DataFrame:
    return pd.read_csv(fetch_zipped_file("metadata"))


def get_tile_coords() -> pd.DataFrame:
    return pd.read_csv(fetch_zipped_file("tile_coords"))


def get_region_names() -> pd.DataFrame:
    return pd.read_csv(fetch_zipped_file("region_names"))


def get_tile_urls() -> pd.DataFrame:
    return pd.read_csv(fetch_zipped_file("tile_urls"))

# %% ../notebooks/00_io.ipynb 12
def normalize_tile_id(tile_id: str) -> str:
    """Normalize a tile ID by adding 'APF' prefix and leading zeros if necessary.

    Parameters
    ----------
    tile_id : str
        Full or partial tile ID. If partial, it will be padded with 'APF' and leading zeros.

    Returns
    -------
    str
        Complete tile ID in format 'APF0000xxx' (always 9 characters)

    Examples
    --------
    >>> normalize_tile_id('r8y')
    'APF0000r8y'
    >>> normalize_tile_id('0000r8y')
    'APF0000r8y'
    >>> normalize_tile_id('APF0000r8y')
    'APF0000r8y'
    >>> normalize_tile_id('123r8y')
    'APF0123r8y'
    """
    # Remove 'APF' prefix if present
    if tile_id.upper().startswith("APF"):
        tile_id = tile_id[3:]

    # Calculate how many zeros we need to add
    target_length = 7  # length without 'APF'
    current_length = len(tile_id)

    if current_length > target_length:
        raise ValueError(f"Tile ID too long: {tile_id}")

    # Add necessary leading zeros
    padded_id = "0" * (target_length - current_length) + tile_id

    # Add APF prefix
    return f"APF{padded_id}"

# %% ../notebooks/00_io.ipynb 14
def get_subframe(url):
    targetpath = pooch.retrieve(
        url, path=pooch.os_cache("p4tools/tiles"), known_hash=None, progressbar=True
    )
    im = mplimg.imread(targetpath)
    return im

# %% ../notebooks/00_io.ipynb 15
def get_url_for_tile_id(tile_id):
    return get_tile_urls().set_index("tile_id").squeeze().at[normalize_tile_id(tile_id)]


def get_url_for_tile(tile_id):
    # alias for get_url_for_tile_id
    return get_url_for_tile_id(tile_id)

# %% ../notebooks/00_io.ipynb 19
def get_subframe_by_tile_id(tile_id):
    url = get_url_for_tile_id(tile_id)
    return get_subframe(url)


def get_subframe_for_tile(tile_id):
    # alias for get_subframe_by_tile_id for consistency
    return get_subframe_by_tile_id(tile_id)


# %% ../notebooks/00_io.ipynb 21
def get_fans_for_tile(tile_id):
    tile_id = normalize_tile_id(tile_id)
    fans = get_fan_catalog()
    return fans.query("tile_id == @tile_id")

# %% ../notebooks/00_io.ipynb 23
def get_blotches_for_tile(tile_id):
    tile_id = normalize_tile_id(tile_id)
    blotches = get_blotch_catalog()
    return blotches.query("tile_id == @tile_id")

# %% ../notebooks/00_io.ipynb 26
def get_hirise_id_for_tile(tile_id):
    tile_id = normalize_tile_id(tile_id)
    try:
        obsid = get_fan_catalog().query("tile_id == @tile_id").obsid.iloc[0]
    except IndexError:
        try:
            obsid = get_blotch_catalog().query("tile_id == @tile_id").obsid.iloc[0]
        except IndexError:
            raise ValueError(f"No obsid found for tile {tile_id}")
    else:
        return obsid
