{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp production.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IO\n",
    "\n",
    "> IO infrastructure for producing the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export \n",
    "from __future__ import annotations\n",
    "\n",
    "###imports p4tools\n",
    "\n",
    "\n",
    "###imports packages\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import logging\n",
    "import configparser\n",
    "import dask.dataframe as dd\n",
    "\n",
    "###imports typing\n",
    "from configparser import ConfigParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "pkg_name = __name__.split(\".\")[0]\n",
    "configpath = Path.home() / \".{}.ini\".format(pkg_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configuration file /home/tihro/.__main__.ini found.\n",
      "\n",
      "Saved database path into /home/tihro/.__main__.ini.\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "from configparser import ConfigParser\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_config() -> ConfigParser:\n",
    "    \"\"\"Read the configfile and return config dict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ConfigParser\n",
    "        Dictionary with the content of the configpath file.\n",
    "    \"\"\"\n",
    "    if not configpath.exists():\n",
    "        raise IOError(\"Config file {} not found.\".format(str(configpath)))\n",
    "    else:\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(str(configpath))\n",
    "        return config\n",
    "\n",
    "\n",
    "def set_database_path(dbfolder) -> None:\n",
    "    \"\"\"Use to write the database path into the config.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dbfolder : str or pathlib.Path\n",
    "        Path to where planet4 will store clustering results by default.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        d = get_config()\n",
    "    except IOError:\n",
    "        d = configparser.ConfigParser()\n",
    "        d[\"planet4_db\"] = {}\n",
    "    d[\"planet4_db\"][\"path\"] = dbfolder\n",
    "    with configpath.open(\"w\") as f:\n",
    "        d.write(f)\n",
    "    print(\"Saved database path into {}.\".format(configpath))\n",
    "\n",
    "\n",
    "def get_data_root() -> Path:\n",
    "    d = get_config()\n",
    "    data_root = Path(d[\"planet4_db\"][\"path\"]).expanduser()\n",
    "    data_root.mkdir(exist_ok=True, parents=True)\n",
    "    return data_root\n",
    "\n",
    "\n",
    "def get_ground_projection_root() -> Path | None:\n",
    "    d = get_config()\n",
    "    try:\n",
    "        gp_root = Path(d[\"ground_projection\"][\"path\"])\n",
    "    except KeyError:\n",
    "        # warnings.warn(\n",
    "        #     \"ground_projection_root not set in config.\\n\"\n",
    "        #     \"Read/Write of projected data is disabled.\"\n",
    "        # )\n",
    "        gp_root = None\n",
    "    else:\n",
    "        gp_root.mkdir(exist_ok=True)\n",
    "    return gp_root\n",
    "\n",
    "\n",
    "if not configpath.exists():\n",
    "    print(\"No configuration file {} found.\\n\".format(configpath))\n",
    "    savepath = input(\"Please provide the path where you want to store planet4 results:\")\n",
    "    set_database_path(savepath)\n",
    "else:\n",
    "    data_root = get_data_root()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export \n",
    "\n",
    "def check_and_pad_id(imgid)-> str | None:\n",
    "    \"\"\"Checks the Image ID and pads it if necessary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgid : None | str\n",
    "        The ID of the individual image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None | str\n",
    "        The padded image ID if it was provided, otherwise None.\n",
    "    \"\"\"\n",
    "    if imgid is None:\n",
    "        return None\n",
    "    imgid_template = \"APF0000000\"\n",
    "    if len(imgid) < len(imgid_template):\n",
    "        imgid = imgid_template[: -len(imgid)] + imgid\n",
    "    return imgid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export \n",
    "\n",
    "class PathManager:\n",
    "\n",
    "    \"\"\"Manage file paths and folders related to the analysis pipeline.\n",
    "\n",
    "    Level definitions:\n",
    "    * L0 : Raw output of Planet Four\n",
    "    * L1A : Clustering of Blotches and Fans on their own\n",
    "    * L1B : Clustered blotches and fans combined into final fans, final blotches, and fnotches that\n",
    "    need to have a cut applied for the decision between fans or blotches.\n",
    "    * L1C : Derived database where a cut has been applied for fnotches to become either fan or\n",
    "    blotch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id_ : str, optional\n",
    "        The data item id that is used to determine sub-paths. Can be set after\n",
    "        init.\n",
    "    datapath : str or pathlib.Path, optional\n",
    "        the base path from where to manage all derived paths. No default assumed\n",
    "        to prevent errors.\n",
    "    suffix : {'.hdf', '.h5', '.csv'}\n",
    "        The suffix that controls the reader function to be used.\n",
    "    obsid : str, optional\n",
    "        HiRISE obsid (i.e. P4 image_name), added as a folder inside path.\n",
    "        Can be set after init.\n",
    "    extra_path : str, pathlib.Path, optional\n",
    "        Any extra path element that needs to be added to the standard path.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cut_dir : pathlib.Path\n",
    "        Defined in `get_cut_folder`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        id_=\"\",\n",
    "        datapath=\"clustering\",\n",
    "        suffix=\".csv\",\n",
    "        obsid=\"\",\n",
    "        cut=0.5,\n",
    "        extra_path=\"\",\n",
    "    ):\n",
    "        self.id = id_\n",
    "        self.cut = cut\n",
    "        self._obsid = obsid\n",
    "        self.extra_path = extra_path\n",
    "\n",
    "        if datapath is None:\n",
    "            # take default path if none given\n",
    "            self._datapath = Path(data_root) / \"clustering\"\n",
    "        elif Path(datapath).is_absolute():\n",
    "            # if given datapath is absolute, take only that:\n",
    "            self._datapath = Path(datapath)\n",
    "        else:\n",
    "            # if it is relative, add it to data_root\n",
    "            self._datapath = Path(data_root) / datapath\n",
    "        self.suffix = suffix\n",
    "\n",
    "        # point reader to correct function depending on required suffix\n",
    "        if suffix in [\".hdf\", \".h5\"]:\n",
    "            self.reader = pd.read_hdf\n",
    "        elif suffix == \".csv\":\n",
    "            self.reader = pd.read_csv\n",
    "\n",
    "        # making sure to warn the user here if the data isn't where it's expected to be\n",
    "        if id_ != \"\":\n",
    "            if not self.path_so_far.exists():\n",
    "                raise FileNotFoundError(f\"{self.path_so_far} does not exist.\")\n",
    "\n",
    "    @property\n",
    "    def id(self):\n",
    "        return self._id\n",
    "\n",
    "    @id.setter\n",
    "    def id(self, value):\n",
    "        if value is not None:\n",
    "            self._id = check_and_pad_id(value)\n",
    "\n",
    "    @property\n",
    "    def clustering_logfile(self):\n",
    "        return self.fanfile.parent / \"clustering_settings.yaml\"\n",
    "\n",
    "    @property\n",
    "    def obsid(self):\n",
    "        if self._obsid == \"\":\n",
    "            if self.id != \"\":\n",
    "                LOGGER.debug(\"Entering obsid search for known image_id.\")\n",
    "                db = DBManager()\n",
    "                df = db.read(columns=[\"image_id\", \"image_name\"])\n",
    "                obsid = df.query(\"image_id==@self.id\").image_name.iloc[0]\n",
    "                LOGGER.debug(\"obsid found: %s\", obsid)\n",
    "                self._obsid = obsid\n",
    "        return self._obsid\n",
    "\n",
    "    @obsid.setter\n",
    "    def obsid(self, value):\n",
    "        self._obsid = value\n",
    "\n",
    "    @property\n",
    "    def obsid_results_savefolder(self):\n",
    "\n",
    "        if self.datapath == None:\n",
    "            raise ValueError(\"No Datapath Suplied\")\n",
    "        else:\n",
    "            savefolder = self.datapath\n",
    "        \n",
    "        savefolder.mkdir(exist_ok=True, parents=True)\n",
    "        return savefolder\n",
    "\n",
    "    @property\n",
    "    def obsid_final_fans_path(self):\n",
    "        return self.obsid_results_savefolder / f\"{self.obsid}_fans.csv\"\n",
    "\n",
    "    @property\n",
    "    def obsid_final_blotches_path(self):\n",
    "        return self.obsid_results_savefolder / f\"{self.obsid}_blotches.csv\"\n",
    "\n",
    "    @property\n",
    "    def datapath(self):\n",
    "        return self._datapath\n",
    "\n",
    "    @property\n",
    "    def path_so_far(self):\n",
    "        p = self.datapath\n",
    "        p /= self.extra_path\n",
    "        p /= self.obsid\n",
    "        return p\n",
    "\n",
    "    @property\n",
    "    def L1A_folder(self):\n",
    "        \"Subfolder name for the clustered data before fnotching.\"\n",
    "        return \"L1A\"\n",
    "\n",
    "    @property\n",
    "    def L1B_folder(self):\n",
    "        \"Subfolder name for the fnotched data, before cut is applied.\"\n",
    "        return \"L1B\"\n",
    "\n",
    "    @property\n",
    "    def L1C_folder(self):\n",
    "        \"subfolder name for the final catalog after applying `cut`.\"\n",
    "        return \"L1C_cut_{:.1f}\".format(self.cut)\n",
    "\n",
    "    def get_path(self, marking, specific=\"\")-> Path:\n",
    "        \n",
    "        p = self.path_so_far\n",
    "        # now add the image_id\n",
    "        try:\n",
    "            p /= self.id\n",
    "        except TypeError:\n",
    "            logging.warning(\"self.id not set. Storing in obsid level.\")\n",
    "\n",
    "        id_ = self.id if self.id != \"\" else self.obsid\n",
    "\n",
    "        # add the specific sub folder\n",
    "        p /= specific\n",
    "\n",
    "        if specific != \"\":\n",
    "            p /= f\"{id_}_{specific}_{marking}{self.suffix}\"\n",
    "        else:\n",
    "            # prepend the data level to file name if given.\n",
    "            p /= f\"{id_}_{marking}{self.suffix}\"\n",
    "        return p\n",
    "\n",
    "    def get_obsid_paths(self, level):\n",
    "        \"\"\"get all existing paths for a given data level.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        level : {'L1A', 'L1B', 'L1C'}\n",
    "        \"\"\"\n",
    "        folder = self.path_so_far\n",
    "        # cast to upper case for the lazy... ;)\n",
    "        level = level.upper()\n",
    "        image_id_paths = [item for item in folder.glob(\"*\") if item.is_dir()]\n",
    "        bucket = []\n",
    "        for p in image_id_paths:\n",
    "            try:\n",
    "                bucket.append(next(p.glob(f\"{level}*\")))\n",
    "            except StopIteration:\n",
    "                continue\n",
    "        return bucket\n",
    "\n",
    "    def get_df(self, fpath):\n",
    "        return self.reader(str(fpath))\n",
    "\n",
    "    @property\n",
    "    def fanfile(self):\n",
    "        return self.get_path(\"fans\", self.L1A_folder)\n",
    "\n",
    "    @property\n",
    "    def fandf(self):\n",
    "        return self.get_df(self.fanfile)\n",
    "\n",
    "    @property\n",
    "    def reduced_fanfile(self):\n",
    "        return self.get_path(\"fans\", self.L1B_folder)\n",
    "\n",
    "    @property\n",
    "    def reduced_fandf(self):\n",
    "        return self.get_df(self.reduced_fanfile)\n",
    "\n",
    "    @property\n",
    "    def final_fanfile(self):\n",
    "        return self.get_path(\"fans\", self.L1C_folder)\n",
    "\n",
    "    @property\n",
    "    def final_fandf(self):\n",
    "        return self.get_df(self.final_fanfile)\n",
    "\n",
    "    @property\n",
    "    def blotchfile(self):\n",
    "        return self.get_path(\"blotches\", self.L1A_folder)\n",
    "\n",
    "    @property\n",
    "    def blotchdf(self):\n",
    "        return self.get_df(self.blotchfile)\n",
    "\n",
    "    @property\n",
    "    def reduced_blotchfile(self):\n",
    "        return self.get_path(\"blotches\", self.L1B_folder)\n",
    "\n",
    "    @property\n",
    "    def reduced_blotchdf(self):\n",
    "        return self.get_df(self.reduced_blotchfile)\n",
    "\n",
    "    @property\n",
    "    def final_blotchfile(self):\n",
    "        return self.get_path(\"blotches\", self.L1C_folder)\n",
    "\n",
    "    @property\n",
    "    def final_blotchdf(self):\n",
    "        return self.get_df(self.final_blotchfile)\n",
    "\n",
    "    @property\n",
    "    def fnotchfile(self):\n",
    "        return self.get_path(\"fnotches\", self.L1B_folder)\n",
    "\n",
    "    @property\n",
    "    def fnotchdf(self):\n",
    "        # the fnotchfile has an index, so i need to read that here:\n",
    "        return pd.read_csv(self.fnotchfile, index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export \n",
    "\n",
    "class DBManager:\n",
    "\n",
    "    \"\"\"Access class for database activities.\n",
    "\n",
    "    Provides easy access to often used data items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dbname : str, optional\n",
    "        Path to database file to be used. Default: use get_latest_cleaned_db() to\n",
    "        find it.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    image_names\n",
    "    image_ids\n",
    "    n_image_ids\n",
    "    n_image_names\n",
    "    obsids : Alias to image_ids\n",
    "    season2and3_image_names\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dbname=None):\n",
    "        \"\"\"Initialize DBManager class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dbname : <str>\n",
    "            Filename of database file to use. Default: Latest produced full\n",
    "            database.\n",
    "        \"\"\"\n",
    "        if dbname is None:\n",
    "            self.dbname = Path(get_latest_cleaned_db())\n",
    "        else:\n",
    "            self.dbname = Path(dbname)\n",
    "        self.df = dd.read_parquet(self.dbname)\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"Database root: {}\\n\".format(Path(self.dbname).parent)\n",
    "        s += \"Database name: {}\\n\".format(Path(self.dbname).name)\n",
    "        return s\n",
    "\n",
    "    def read(self, **kwargs):\n",
    "        p = Path(self.dbname)\n",
    "        if p.suffix.endswith(\"hdf\"):\n",
    "            return pd.read_hdf(p, **kwargs)\n",
    "        elif p.suffix.endswith(\"parquet\"):\n",
    "            where = kwargs.pop(\"where\", None)\n",
    "            if where is not None:\n",
    "                obsid = where.split(\"=\")[-1].strip()\n",
    "                folder = p.parent / p.stem\n",
    "                fname = (folder / obsid).with_suffix(\".parquet\")\n",
    "                return pd.read_parquet(fname)\n",
    "            else:\n",
    "                return pd.read_parquet(p, **kwargs)\n",
    "        elif p.suffix.endswith(\"csv\"):\n",
    "            return pd.read_csv(p, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def orig_csv(self):\n",
    "        p = self.dbname\n",
    "        return p.parent / (p.name[:38] + \".csv\")\n",
    "\n",
    "    def get_obsid_for_tile_id(self, tile_id):\n",
    "        tile_id = check_and_pad_id(tile_id)\n",
    "        obsid = self.df[self.df.image_id == tile_id].image_name.compute().iloc[0]\n",
    "        return obsid\n",
    "\n",
    "    def set_latest_with_dupes_db(self, datadir=None):\n",
    "        datadir = data_root if datadir is None else Path(datadir)\n",
    "        h5files = datadir.glob(\"201*_queryable.h5\")\n",
    "        dbname = get_latest_file(h5files)\n",
    "        print(\"Setting {} as dbname.\".format(dbname.name))\n",
    "        self.dbname = Path(dbname)\n",
    "\n",
    "    @property\n",
    "    def image_names(self):\n",
    "        \"\"\"Return list of unique obsids used in database.\n",
    "\n",
    "        See also\n",
    "        --------\n",
    "        get_image_names_from_db\n",
    "        \"\"\"\n",
    "        return self.df.image_name.unique().compute()\n",
    "\n",
    "    @property\n",
    "    def image_ids(self):\n",
    "        \"Return list of unique image_ids in database.\"\n",
    "        return self.df.image_id.unique().compute()\n",
    "\n",
    "    @property\n",
    "    def n_image_ids(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    @property\n",
    "    def n_image_names(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    @property\n",
    "    def obsids(self):\n",
    "        \"Alias to self.image_names.\"\n",
    "        return self.image_names\n",
    "\n",
    "    def get_obsid_markings(self, obsid):\n",
    "        \"Return marking data for given HiRISE obsid.\"\n",
    "        return self.df[self.df.image_name == obsid].compute()\n",
    "\n",
    "    def get_image_name_markings(self, image_name):\n",
    "        \"Alias for get_obsid_markings.\"\n",
    "        return self.get_obsid_markings(image_name)\n",
    "\n",
    "    def get_image_id_markings(self, image_id, obsid=None):\n",
    "        \"Return marking data for one Planet4 image_id\"\n",
    "        image_id = check_and_pad_id(image_id)\n",
    "        if obsid is None:\n",
    "            obsid = self.get_obsid_for_tile_id(image_id)\n",
    "        data = self.get_image_name_markings(obsid)\n",
    "        return data.query(\"image_id==@image_id\")\n",
    "\n",
    "    def get_data_for_obsids(self, obsids):\n",
    "        bucket = []\n",
    "        for obsid in obsids:\n",
    "            bucket.append(self.get_obsid_markings(obsid))\n",
    "        return pd.concat(bucket, ignore_index=True)\n",
    "\n",
    "    def get_classification_id_data(self, class_id):\n",
    "        \"Return data for one classification_id\"\n",
    "        return self.read(where=\"classification_id=='{}'\".format(class_id))\n",
    "\n",
    "    @property\n",
    "    def season2and3_image_names(self):\n",
    "        \"numpy.array : List of image_names for season 2 and 3.\"\n",
    "        image_names = self.image_names\n",
    "        metadf = pd.DataFrame(\n",
    "            pd.Series(image_names).astype(\"str\"), columns=[\"image_name\"]\n",
    "        )\n",
    "        stats.define_season_column(metadf)\n",
    "        return metadf[(metadf.season > 1) & (metadf.season < 4)].image_name.unique()\n",
    "\n",
    "    def get_general_filter(self, f):\n",
    "        return self.read(where=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planetary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
