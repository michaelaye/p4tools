{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp production.dbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## production.dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from p4tools.production import io,markings\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import circmean, circstd\n",
    "import pyaml\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def get_average_objects(clusters, kind):\n",
    "    \"\"\"Create the average object out of a sequence of clusters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clusters : sequence of pandas.DataFrames\n",
    "        table with rows of markings (fans or blotches) to be averaged\n",
    "    kind : {'fan', 'blotch}\n",
    "        Switch to control the circularity for the average angle calculation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Generator providing single row pandas.DataFrames with the average values\n",
    "    \"\"\"\n",
    "    logger.debug(\"Averaging clusters with kind = %s.\", kind)\n",
    "    for cluster_df in clusters:\n",
    "        # first filter for outliers more than 1 std away\n",
    "        # for\n",
    "        # reduced = df[df.apply(lambda x: np.abs(x - x.mean()) / x.std() < 1).all(axis=1)]\n",
    "        logger.debug(\"Averaging %i objects.\", len(cluster_df))\n",
    "        logger.debug(\"x.mean: %f\", cluster_df.x.mean())\n",
    "        logger.debug(\"y.mean: %f\", cluster_df.y.mean())\n",
    "        meandata = cluster_df.mean(numeric_only=True)\n",
    "        # this determines the upper limit for circular mean\n",
    "        high = 180 if kind == \"blotch\" else 360\n",
    "        meandata.angle = circmean(cluster_df.angle, high=high)\n",
    "        meandata[\"angle_std\"] = circstd(cluster_df.angle, high=high)\n",
    "        meandata[\"n_votes\"] = len(cluster_df)\n",
    "        meandata[\"x_std\"] = cluster_df.x.std()\n",
    "        meandata[\"y_std\"] = cluster_df.y.std()\n",
    "        if kind == \"fan\":\n",
    "            meandata[\"distance_std\"] = cluster_df.distance.std()\n",
    "            meandata[\"spread_std\"] = cluster_df.spread.std()\n",
    "        elif kind == 'blotch':\n",
    "            meandata[\"radius1_std\"] = cluster_df.radius_1.std()\n",
    "            meandata[\"radius2_std\"] = cluster_df.radius_2.std()\n",
    "\n",
    "        yield meandata.to_frame().T\n",
    "\n",
    "\n",
    "\n",
    "def plot_results(p4id, labels, data=None, kind=None, reduced_data=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plots the results of a clustering algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    p4id : object\n",
    "        An object that contains methods for plotting and showing subframes.\n",
    "    labels : array-like\n",
    "        Cluster labels for each point in the dataset.\n",
    "    data : pandas.DataFrame, optional\n",
    "        The original data points with 'x' and 'y' coordinates. Default is None.\n",
    "    kind : str, optional\n",
    "        The type of marking to plot (e.g., 'blotch', 'fan'). Default is None.\n",
    "    reduced_data : pandas.DataFrame, optional\n",
    "        The reduced data points to be plotted. Default is None.\n",
    "    ax : matplotlib.axes.Axes, optional\n",
    "        The axes on which to plot. If None, a new figure and axes are created. Default is None.\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    functions = dict(blotch=p4id.plot_blotches, fan=p4id.plot_fans)\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    plot_kwds = {\"alpha\": 0.8, \"s\": 10, \"linewidths\": 0}\n",
    "    palette = sns.color_palette(\"bright\", len(labels))\n",
    "    cluster_colors = [palette[x] if x >= 0 else (0.75, 0.75, 0.75) for x in labels]\n",
    "    p4id.show_subframe(ax=ax)\n",
    "    if data is not None:\n",
    "        ax.scatter(data.loc[:, \"x\"], data.loc[:, \"y\"], c=cluster_colors, **plot_kwds)\n",
    "    markings.set_subframe_size(ax)\n",
    "    # pick correct function for kind of marking:\n",
    "    if any(reduced_data):\n",
    "        functions[kind](ax=ax, data=reduced_data, lw=1, with_center=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export \n",
    "class DBScanner:\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    msf : float\n",
    "        m_ean s_amples f_actor: Factor to multiply number of markings with to calculate the\n",
    "        min_samples value for DBSCAN to use\n",
    "    savedir : str, pathlib.Path\n",
    "        Path where to store clustered results\n",
    "    with_angles, with_radii : bool\n",
    "        Switches to control if clustering should include angles and radii respectively.\n",
    "    do_large_run : bool\n",
    "        Switch to control if a second run with parameters set for large objects should\n",
    "        be done.\n",
    "    save_results : bool\n",
    "        Switch to control if the resulting clustered objects should be written to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        msf=0.13,\n",
    "        savedir=None,\n",
    "        with_angles=True,\n",
    "        with_radii=True,\n",
    "        do_large_run=True,\n",
    "        save_results=True,\n",
    "        only_core_samples=False,\n",
    "        data=None,\n",
    "        dbname=None,\n",
    "    ):\n",
    "        self.msf = msf\n",
    "        self.savedir = savedir\n",
    "        self.with_angles = with_angles\n",
    "        self.with_radii = with_radii\n",
    "        self.do_large_run = do_large_run\n",
    "        self.save_results = save_results\n",
    "        self.only_core_samples = only_core_samples\n",
    "        self.data = data\n",
    "        self.pm = io.PathManager(datapath=savedir)\n",
    "        self.noise = []\n",
    "        self.dbname = dbname\n",
    "\n",
    "        # This needs to be on instance level, so that a new object always has these default numbers\n",
    "        # It sets all the different eps values for the different clustering loops here:\n",
    "        self.eps_values = {\n",
    "            \"fan\": {\n",
    "                \"xy\": {\"small\": 10, \"large\": 25},  # in pixels\n",
    "                \"angle\": 20,  # degrees\n",
    "                \"radius\": {\n",
    "                    \"small\": None,  # not in use currently for fans`\n",
    "                    \"large\": None,  # ditto\n",
    "                },\n",
    "            },\n",
    "            \"blotch\": {\n",
    "                \"xy\": {\"small\": 10, \"large\": 25},  # in pixels\n",
    "                \"angle\": None,  # for now deactivated\n",
    "                \"radius\": {\"small\": 30, \"large\": 50},\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def show_markings(self, id_):\n",
    "        \"\"\"\n",
    "        Displays the markings for a given ID.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        id_ : int or str\n",
    "            The identifier for the markings to be displayed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        p4id = markings.TileID(id_)\n",
    "        p4id.plot_all()\n",
    "\n",
    "    def cluster_any(self, X, eps):\n",
    "        \"\"\"\n",
    "        Perform DBSCAN clustering on the given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input data to be clustered.\n",
    "        eps : float\n",
    "            The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "            \n",
    "        Yields\n",
    "        ------\n",
    "        indices : array-like, shape (n_samples,)\n",
    "            Boolean mask indicating the members of the current cluster.\n",
    "        \"\"\"\n",
    "\n",
    "        logger.debug(\"Clustering any.\")\n",
    "        db = DBSCAN(eps, min_samples=self.min_samples).fit(X)\n",
    "        labels = db.labels_\n",
    "        unique_labels = sorted(set(labels))\n",
    "\n",
    "        core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "        self.n_clusters = len(unique_labels) - (1 if -1 in labels else 0)\n",
    "        logger.debug(\"%i cluster(s) found with:\", self.n_clusters)\n",
    "\n",
    "        self.labels = labels\n",
    "\n",
    "        # loop over unique labels.\n",
    "        for k in unique_labels:\n",
    "            class_member_mask = (labels == k)\n",
    "            if k == -1:\n",
    "                self.noise.append(class_member_mask)\n",
    "                continue\n",
    "            if self.only_core_samples is True:\n",
    "                # this has a potentially large effect and can make the number\n",
    "                # of surviving cluster_members smaller than 3 !\n",
    "                indices = class_member_mask & core_samples_mask\n",
    "            else:\n",
    "                indices = class_member_mask\n",
    "            logger.debug(\"%i members.\", np.count_nonzero(indices))\n",
    "            yield indices\n",
    "\n",
    "    def cluster_xy(self, data, eps):\n",
    "        logger.info(\"Clustering x,y with eps: %i\", eps)\n",
    "        X = data[[\"x\", \"y\"]].values\n",
    "        for cluster_index in self.cluster_any(X, eps):\n",
    "            yield data.loc[cluster_index]\n",
    "\n",
    "    def split_markings_by_size(self, data, limit=210):\n",
    "        kind = data.marking.value_counts()\n",
    "        if len(kind) > 1:\n",
    "            raise TypeError(\"Data had more than 1 marking kind.\")\n",
    "        if kind.index[0] == \"blotch\":\n",
    "            f1 = data.radius_1 > limit\n",
    "            f2 = data.radius_2 > limit\n",
    "            data_large = data[f1 | f2]\n",
    "            data_small = data\n",
    "        else:\n",
    "            f1 = data.distance > limit\n",
    "            data_large = data[f1]\n",
    "            data_small = data[~f1]\n",
    "        return data_small, data_large\n",
    "\n",
    "    def cluster_angles(self, xy_clusters, kind):\n",
    "        cols_to_cluster = dict(blotch=[\"y_angle\"], fan=[\"x_angle\", \"y_angle\"])\n",
    "        eps_degrees = self.eps_values[kind][\"angle\"]\n",
    "        logger.info(\"Clustering angles with eps: %i\", eps_degrees)\n",
    "        # convert to radians\n",
    "        # calculated value of euclidean distance of unit vector\n",
    "        # end points per degree\n",
    "        eps_per_degree = np.pi*2 / 360\n",
    "        eps = eps_degrees * eps_per_degree\n",
    "        for xy_cluster in xy_clusters:\n",
    "            X = xy_cluster[cols_to_cluster[kind]]\n",
    "            for indices in self.cluster_any(X, eps):\n",
    "                yield xy_cluster.loc[indices]\n",
    "\n",
    "    def cluster_radii(self, angle_clusters, eps):\n",
    "        logger.info(\"Clustering radii with eps: %i\", eps)\n",
    "        cols_to_cluster = [\"radius_1\", \"radius_2\"]\n",
    "        for angle_cluster in angle_clusters:\n",
    "            X = angle_cluster[cols_to_cluster]\n",
    "            for indices in self.cluster_any(X, eps):\n",
    "                yield angle_cluster.loc[indices]\n",
    "\n",
    "    def cluster_and_plot(\n",
    "        self,\n",
    "        img_id,\n",
    "        kind,\n",
    "        msf=None,\n",
    "        eps_values=None,\n",
    "        ax=None,\n",
    "        fontsize=None,\n",
    "        saveplot=True,\n",
    "    ):\n",
    "        \"\"\"Cluster and plot the results for one P4 image_id.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img_ig : str\n",
    "            Planet Four image_id\n",
    "        kind : {'fan', 'blotch'}\n",
    "            Kind of marking\n",
    "        eps_values : dictionary, optional\n",
    "            Dictionary with clustering values to be used. If not given, use stored default one.\n",
    "            This is mostly used for `self.parameter_scan`.\n",
    "        ax : matplotlib.axis, optional\n",
    "            Matplotlib axis to be used for plotting. If not given, a new figure and axis is\n",
    "            created.\n",
    "        fontsize : int, optional\n",
    "            Fontsize for the plots' headers.\n",
    "        \"\"\"\n",
    "        if msf is not None:\n",
    "            self.msf = msf\n",
    "        if eps_values is None:\n",
    "            # if not given, use stored default values:\n",
    "            eps_values = self.eps_values\n",
    "\n",
    "        self.cluster_image_id(img_id, msf, eps_values)\n",
    "\n",
    "        reduced_data = self.reduced_data[kind]\n",
    "\n",
    "        try:\n",
    "            n_reduced = len(reduced_data)\n",
    "        except TypeError:\n",
    "            n_reduced = 0\n",
    "\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        else:\n",
    "            fig = ax.get_figure()\n",
    "        if n_reduced > 0:\n",
    "            plot_results(\n",
    "                self.p4id, self.labels, kind=kind, reduced_data=reduced_data, ax=ax\n",
    "            )\n",
    "        else:\n",
    "            self.p4id.show_subframe(ax=ax)\n",
    "        eps = eps_values[kind][\"xy\"][\"small\"]\n",
    "        eps_large = eps_values[kind][\"xy\"][\"large\"]\n",
    "        ax.set_title(\n",
    "            f\"ID: {img_id}, \"\n",
    "            f\"n: {n_reduced}\\n\"\n",
    "            f\"MS: {self.min_samples}, \"\n",
    "            f\"EPS: {eps}, \"\n",
    "            f\"EPS_LARGE: {eps_large}\",\n",
    "            fontsize=fontsize,\n",
    "        )\n",
    "        if saveplot:\n",
    "            savepath = f\"plots/{img_id}_{kind}_eps{eps}_epsLARGE{eps_large}.png\"\n",
    "            Path(savepath).parent.mkdir(exist_ok=True)\n",
    "            fig.savefig(savepath, dpi=200)\n",
    "\n",
    "    @property\n",
    "    def min_samples(self):\n",
    "        \"\"\"Calculate min_samples for DBSCAN.\n",
    "\n",
    "        From current self.msf value and no of classifications.\n",
    "        \"\"\"\n",
    "        min_samples = round(self.msf * self.p4id.n_marked_classifications)\n",
    "        return max(3, min_samples)  # never use less than 3\n",
    "\n",
    "    def setup_logfiles(self):\n",
    "        if len(logger.handlers) > 0:\n",
    "            for handler in logger.handlers:\n",
    "                if isinstance(handler, logging.FileHandler):\n",
    "                    logger.debug(\"Found logging.FileHandler\")\n",
    "                    return\n",
    "        logpath = self.pm.path_so_far / \"clustering.log\"\n",
    "        logpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "        fh = logging.FileHandler(logpath, \"w\")\n",
    "        formatter = logging.Formatter(\n",
    "            \"%(asctime)s - %(name)s - %(levelname)s\" \" - %(message)s\",\n",
    "            \"%Y-%m-%d %H:%M:%S\",\n",
    "        )\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(fh)\n",
    "        # logger.setLevel(logging.INFO)\n",
    "\n",
    "    def cluster_image_name(self, image_name, msf=None, eps_values=None):\n",
    "        \"Cluster all image_ids for a given image_name (i.e. HiRISE obsid)\"\n",
    "        if msf is not None:\n",
    "            self.msf = msf\n",
    "        self.pm.obsid = image_name\n",
    "        self.setup_logfiles()\n",
    "\n",
    "        logger.info(\"Clustering image_name %s with msf of %f.\", image_name, self.msf)\n",
    "        db = io.DBManager(self.dbname, obsid = image_name)\n",
    "        data = db.get_obsid_markings(image_name)\n",
    "        image_ids = data.image_id.unique()\n",
    "        logger.debug(\"Number of image_ids found: %i\", len(image_ids))\n",
    "        for image_id in tqdm(image_ids,desc=image_name):\n",
    "            self.pm.id = image_id\n",
    "            self.cluster_image_id(image_id, msf, eps_values, image_name)\n",
    "\n",
    "    def write_settings_file(self, eps_values):\n",
    "        eps_values[\"min_samples\"] = self.min_samples\n",
    "        eps_values[\"only_core_samples\"] = self.only_core_samples\n",
    "        settingspath = self.pm.blotchfile.parent / \"clustering_settings.yaml\"\n",
    "        settingspath.parent.mkdir(exist_ok=True, parents=True)\n",
    "        logger.info(\"Writing settings file at %s\", str(settingspath))\n",
    "        with open(settingspath, \"w\") as fp:\n",
    "            pyaml.dump(eps_values, fp)\n",
    "\n",
    "    def cluster_image_id(self, img_id, msf=None, eps_values=None, image_name=None):\n",
    "        \"\"\"Interface function for users to cluster data for one P4 image_id.\n",
    "\n",
    "        This method does the data splitting in case it is required and calls the\n",
    "        `_setup_and_call_clustering` that goes over all dimensions to cluster.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img_ig : str\n",
    "            Planet Four image_id\n",
    "        msf : float, optional\n",
    "            mean_samples_factor to be used for calculating min_samples. Default as given\n",
    "            during __init__.\n",
    "        eps_values : dictionary, optional\n",
    "            Dict with eps values for clustering, in the format as given in `self.eps_values`.\n",
    "            If not provided, the default stored `self.eps_values` is used.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        At the end the data from differently-sized clustering is concatenated into the same\n",
    "        results pd.DataFrame and then being stored per marking kind in the dictionary\n",
    "        `self.reduced_data`.\n",
    "        \"\"\"\n",
    "        self.p4id = markings.TileID(\n",
    "            img_id, scope=\"p4tools\", dbname=self.dbname, data=self.data, image_name=image_name\n",
    "        )\n",
    "        self.pm.obsid = self.p4id.image_name\n",
    "        self.pm.id = img_id\n",
    "\n",
    "        # this will setup the logfile if we have not been called via image_name\n",
    "        # clustering already.\n",
    "        self.setup_logfiles()\n",
    "\n",
    "        logger.info(\"Clustering id: %s with min_samples: %i\", img_id, self.min_samples)\n",
    "        if msf is not None:\n",
    "            # this sets the stored msf, automatically changing min_samples accordingly\n",
    "            self.msf = msf\n",
    "\n",
    "        eps_values = self.eps_values if eps_values is None else eps_values\n",
    "        self.write_settings_file(eps_values)\n",
    "        # set up storage for results\n",
    "        self.reduced_data = {}\n",
    "        self.final_clusters = {}\n",
    "        old_val = self.do_large_run\n",
    "        for kind in [\"fan\", \"blotch\"]:\n",
    "            logger.info(\"Working on %s.\", kind)\n",
    "            self.current_kind = kind\n",
    "            if kind == \"fan\":\n",
    "                self.do_large_run = False\n",
    "            else:\n",
    "                self.do_large_run = old_val\n",
    "            # fill in empty lists in case we need to bail for not enough data\n",
    "            self.final_clusters[kind] = []\n",
    "            self.reduced_data[kind] = []\n",
    "            # Receive the fans or blotches, respectively:\n",
    "            data = self.p4id.filter_data(kind)\n",
    "            if len(data) < self.min_samples:\n",
    "                # skip all else if we have not enough markings\n",
    "                continue\n",
    "            # cluster first with the parameters for small objects\n",
    "            self._setup_and_call_clustering(eps_values, data, kind, \"small\")\n",
    "            # self.remaining was created during previous call.\n",
    "            if len(self.remaining) > self.min_samples and self.do_large_run is True:\n",
    "                # if we allow it, and more than min_samples are left, do 2nd round\n",
    "                # with parameters for large objects\n",
    "                logger.info(\"Clustering on remaining data with large parameter set.\")\n",
    "                self._setup_and_call_clustering(\n",
    "                    eps_values, self.remaining, kind, \"large\"\n",
    "                )\n",
    "            # merging small and large clustering results\n",
    "            try:\n",
    "                self.reduced_data[kind] = pd.concat(\n",
    "                    self.reduced_data[kind], ignore_index=True, sort=True\n",
    "                )\n",
    "            except ValueError:\n",
    "                # i can just continue here, as I stored an empty list above already\n",
    "                continue\n",
    "\n",
    "        if self.save_results:\n",
    "            self.store_clustered(self.reduced_data)\n",
    "\n",
    "    def _setup_and_call_clustering(self, eps_values, dataset, kind, size):\n",
    "        \"\"\"setup helper for the clustering pipeline.\n",
    "\n",
    "        This just reads out the values from the eps_values structure and then calls\n",
    "        `_cluster_pipeline`.\n",
    "        \"\"\"\n",
    "        logger.info(\"Processing %s dataset.\", size)\n",
    "        eps_xy = eps_values[kind][\"xy\"][size]\n",
    "        eps_rad = eps_values[kind][\"radius\"][size]\n",
    "        logger.debug(\"Length of dataset: %i\", len(dataset))\n",
    "        self.reduced_data[kind].append(\n",
    "            self._cluster_pipeline(kind, dataset, eps_xy, eps_rad)\n",
    "        )\n",
    "        logger.debug(\"Appending %i items to final_clusters\", len(self.finalclusters))\n",
    "        self.final_clusters[kind].append(self.finalclusters)\n",
    "\n",
    "    def _calculate_unclustered(self, data, xyclusters):\n",
    "        data_in = data.dropna(how=\"all\", axis=1)\n",
    "        try:\n",
    "            clustered = pd.concat(xyclusters).dropna(how=\"all\", axis=1)\n",
    "        except ValueError:\n",
    "            self.remaining = []\n",
    "        else:\n",
    "            self.remaining = data_in[~data_in.isin(clustered).all(1)]\n",
    "        if self.current_kind == \"blotch\" and len(self.remaining) > 0:\n",
    "            eps = 0.00001\n",
    "            blotch_defaults = ((self.remaining.radius_1 - 10) < eps) & (\n",
    "                (self.remaining.radius_2 - 10).abs() < eps\n",
    "            )\n",
    "            self.remaining = self.remaining[~blotch_defaults]\n",
    "\n",
    "    def _cluster_pipeline(self, kind, data, eps, eps_rad):\n",
    "        \"\"\"Cluster pipeline that can cluster over xy, angles and radii.\n",
    "\n",
    "        It does so without knowledge of different marking sizes, it just receives data and\n",
    "        will cluster it together, successively.\n",
    "        \"\"\"\n",
    "        xyclusters = self.cluster_xy(data, eps)\n",
    "        xyclusters = list(xyclusters)\n",
    "        self._calculate_unclustered(data, xyclusters)\n",
    "        if self.with_radii and eps_rad is not None:\n",
    "            last = self.cluster_radii(xyclusters, eps_rad)\n",
    "        else:\n",
    "            last = xyclusters\n",
    "        last = list(last)\n",
    "        if self.with_angles and self.eps_values[kind][\"angle\"] is not None:\n",
    "            finalclusters = self.cluster_angles(last, kind)\n",
    "        else:\n",
    "            finalclusters = last\n",
    "        last = list(last)\n",
    "        finalclusters = list(finalclusters)\n",
    "        self.finalclusters = finalclusters\n",
    "        averaged = get_average_objects(finalclusters, kind)\n",
    "        try:\n",
    "            reduced_data = pd.concat(averaged, ignore_index=True, sort=True)\n",
    "        except ValueError as e:\n",
    "            if e.args[0].startswith(\"No objects to concatenate\"):\n",
    "                # logger.warning(\"No clusters survived.\")\n",
    "                return None\n",
    "            else:\n",
    "                raise e\n",
    "        return reduced_data\n",
    "\n",
    "    def parameter_scan(\n",
    "        self,\n",
    "        img_id,\n",
    "        kind,\n",
    "        msf_vals_to_scan,\n",
    "        eps_vals_to_scan,\n",
    "        size_to_scan=\"large\",\n",
    "        do_scale=False,\n",
    "        create_plot=True,\n",
    "    ):\n",
    "        \"\"\"Method to scan parameter space and plot results in multi-figure plot.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        kind : {'fan', 'blotch'}\n",
    "            Marking kind\n",
    "        msf_values : iterable (list, array, tuple), length of 2\n",
    "            1D container for msf values to use\n",
    "        eps_values : iterable, length of 3\n",
    "            1D container for eps_values to be used. If they are used for the small or large\n",
    "            items is determined by `size_to_scan`\n",
    "        size_to_scan : {'small', 'large'}\n",
    "            Switch to interpret which eps_values I have received. If 'small' to scan, I take\n",
    "            the large value from `self.eps_values` as constant, and vice versa.\n",
    "        do_scale : bool\n",
    "            Switch to control if scaling is applied.\n",
    "        \"\"\"\n",
    "        self.kind = kind\n",
    "        fig, ax = plt.subplots(\n",
    "            nrows=len(msf_vals_to_scan),\n",
    "            ncols=len(eps_vals_to_scan) + 1,\n",
    "            figsize=(10, 5),\n",
    "        )\n",
    "        axes = ax.flatten()\n",
    "\n",
    "        for ax, (msf, eps) in zip(axes, product(msf_vals_to_scan, eps_vals_to_scan)):\n",
    "            eps_values = self.eps_values.copy()\n",
    "\n",
    "            eps_values[kind][\"xy\"][size_to_scan] = eps\n",
    "\n",
    "            self.cluster_and_plot(\n",
    "                img_id, kind, msf, eps_values, ax=ax, fontsize=8, saveplot=False\n",
    "            )\n",
    "            t = ax.get_title()\n",
    "            ax.set_title(\"MSF: {}, {}\".format(msf, t), fontsize=8)\n",
    "\n",
    "        # plot input tile\n",
    "        self.p4id.show_subframe(ax=axes[-1])\n",
    "        axes[-1].set_title(\"Input tile\", fontsize=8)\n",
    "        # plot marking data\n",
    "        self.p4id.plot_markings(kind, ax=axes[-2], lw=0.25, with_center=True)\n",
    "        axes[-2].set_title(\"{} marking data\".format(kind), fontsize=8)\n",
    "        fig.suptitle(\n",
    "            \"ID: {}, n_class: {}, angles: {}, radii: {}\".format(\n",
    "                img_id,\n",
    "                self.p4id.n_marked_classifications,\n",
    "                self.with_angles,\n",
    "                self.with_radii,\n",
    "            )\n",
    "        )\n",
    "        if create_plot:\n",
    "            savepath = f\"plots/{img_id}_{kind}_angles{self.with_angles}_radii{self.with_radii}.png\"\n",
    "            Path(savepath).parent.mkdir(exist_ok=True)\n",
    "            fig.savefig(savepath, dpi=200)\n",
    "\n",
    "    @property\n",
    "    def n_clustered_fans(self):\n",
    "        \"\"\"int : Number of clustered fans.\"\"\"\n",
    "        return len(self.reduced_data[\"fan\"])\n",
    "\n",
    "    @property\n",
    "    def n_clustered_blotches(self):\n",
    "        \"\"\"int : Number of clustered blotches.\"\"\"\n",
    "        return len(self.reduced_data[\"blotch\"])\n",
    "\n",
    "    def store_clustered(self, reduced_data):\n",
    "        \"Store the clustered but as of yet unfnotched data.\"\n",
    "\n",
    "        logger.debug(\"Storing reduced_data.\")\n",
    "        # get the PathManager object\n",
    "        pm = self.pm\n",
    "\n",
    "        for outpath, outdata in zip(\n",
    "            [pm.blotchfile, pm.fanfile], [reduced_data[\"blotch\"], reduced_data[\"fan\"]]\n",
    "        ):\n",
    "            outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "            if outpath.exists():\n",
    "                outpath.unlink()\n",
    "            if not any(outdata):\n",
    "                logger.debug(\"No data for %s\", str(outpath))\n",
    "                continue\n",
    "            df = outdata\n",
    "            try:\n",
    "                df[\"n_votes\"] = df[\"n_votes\"].astype(\"int\")\n",
    "                df[\"image_id\"] = self.pm.id\n",
    "                df[\"image_name\"] = self.pm.obsid\n",
    "            # when df is just list of Nones, will create TypeError\n",
    "            # for bad indexing into list.\n",
    "            except TypeError:\n",
    "                # nothing to write\n",
    "                logger.warning(\"Outdata was empty, nothing to store.\")\n",
    "                return\n",
    "            df.to_csv(str(outpath.with_suffix(\".csv\")), index=False)\n",
    "            logger.debug(\"Wrote %s\", str(outpath.with_suffix(\".csv\")))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planetary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
