{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp production.catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# production.catalog\n",
    "\n",
    "> this submodule stores the main functions to create a catalog from the planet4 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# p4tools package imports\n",
    "import p4tools.catalog.io as io\n",
    "import p4tools.catalog.metadata as p4meta\n",
    "from p4tools.catalog.projection import X2LATLON, P4Mosaic, TileCaclulator, create_RED45_mosaic\n",
    "\n",
    "# other imports\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import logging\n",
    "import itertools\n",
    "from planetarypy.pds.apps import get_index\n",
    "\n",
    "#typing imports\n",
    "from collections.abc import Iterable\n",
    "from logging import Logger\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export \n",
    "\n",
    "#starting the logger\n",
    "LOGGER: Logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ReleaseManager:\n",
    "    \"\"\"Class to manage releases and find relevant files.\n",
    "    TODO better description\n",
    "    Parameters\n",
    "    ----------\n",
    "    version : str\n",
    "        Version string for this catalog. Same as datapath in other P4 code.\n",
    "    obsids : iterable, optional\n",
    "        Iterable of obsids that should be used for catalog file. Default is to use the full list of the default database, which is Seasons 2 and 3 at this point.\n",
    "    overwrite : bool, optional\n",
    "        Switch to control if already existing result folders for an obsid should be overwritten.\n",
    "        Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    DROP_FOR_TILE_COORDS: list[str] = [\n",
    "        \"xy_hirise\",\n",
    "        \"SampleResolution\",\n",
    "        \"LineResolution\",\n",
    "        \"PositiveWest360Longitude\",\n",
    "        \"Line\",\n",
    "        \"Sample\",\n",
    "    ]\n",
    "\n",
    "    FAN_COLUMNS_AS_PUBLISHED: list[str] = [\n",
    "        \"marking_id\",\n",
    "        \"angle\",\n",
    "        \"distance\",\n",
    "        \"tile_id\",\n",
    "        \"image_x\",\n",
    "        \"image_y\",\n",
    "        \"n_votes\",\n",
    "        \"obsid\",\n",
    "        \"spread\",\n",
    "        \"version\",\n",
    "        \"vote_ratio\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"x_angle\",\n",
    "        \"y_angle\",\n",
    "        \"l_s\",\n",
    "        \"map_scale\",\n",
    "        \"north_azimuth\",\n",
    "        \"BodyFixedCoordinateX\",\n",
    "        \"BodyFixedCoordinateY\",\n",
    "        \"BodyFixedCoordinateZ\",\n",
    "        \"PlanetocentricLatitude\",\n",
    "        \"PlanetographicLatitude\",\n",
    "        \"Longitude\",\n",
    "    ]\n",
    "    BLOTCH_COLUMNS_AS_PUBLISHED: list[str] = [\n",
    "        \"marking_id\",\n",
    "        \"angle\",\n",
    "        \"tile_id\",\n",
    "        \"image_x\",\n",
    "        \"image_y\",\n",
    "        \"n_votes\",\n",
    "        \"obsid\",\n",
    "        \"radius_1\",\n",
    "        \"radius_2\",\n",
    "        \"vote_ratio\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"x_angle\",\n",
    "        \"y_angle\",\n",
    "        \"l_s\",\n",
    "        \"map_scale\",\n",
    "        \"north_azimuth\",\n",
    "        \"BodyFixedCoordinateX\",\n",
    "        \"BodyFixedCoordinateY\",\n",
    "        \"BodyFixedCoordinateZ\",\n",
    "        \"PlanetocentricLatitude\",\n",
    "        \"PlanetographicLatitude\",\n",
    "        \"Longitude\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, version, obsids=None, overwrite=False, dbname=None):\n",
    "        self.catalog = f\"P4_catalog_{version}\"\n",
    "        self.overwrite = overwrite\n",
    "        self._obsids: Iterable | None = obsids\n",
    "        self.dbname = dbname\n",
    "\n",
    "    @property\n",
    "    def savefolder(self):\n",
    "        \"Path to catalog folder\"\n",
    "        return io.data_root / self.catalog\n",
    "\n",
    "    @property\n",
    "    def metadata_path(self):\n",
    "        \"Path to catalog metadata file.\"\n",
    "        return self.savefolder / f\"{self.catalog}_metadata.csv\"\n",
    "\n",
    "    @property\n",
    "    def tile_coords_path(self):\n",
    "        \"Path to catalog tile coordinates file.\"\n",
    "        return self.savefolder / f\"{self.catalog}_tile_coords.csv\"\n",
    "\n",
    "    @property\n",
    "    def tile_coords_path_final(self):\n",
    "        \"Path to final catalog tile coordinates file.\"\n",
    "        return self.savefolder / f\"{self.catalog}_tile_coords_final.csv\"\n",
    "\n",
    "    @property\n",
    "    def obsids(self):\n",
    "        \"\"\"Return list of obsids for catalog production.\n",
    "\n",
    "        If ._obsids is None, get default full obsids list for current default P4 database.\n",
    "        \"\"\"\n",
    "        if self._obsids is None:\n",
    "            db = io.DBManager(dbname=self.dbname)\n",
    "            self._obsids = db.obsids\n",
    "        return self._obsids\n",
    "\n",
    "    @obsids.setter\n",
    "    def obsids(self, values):\n",
    "        self._obsids = values\n",
    "\n",
    "    @property\n",
    "    def fan_file(self):\n",
    "        \"Return path to fan catalog file.\"\n",
    "        try:\n",
    "            return next(self.savefolder.glob(\"*_fan.csv\"))\n",
    "        except StopIteration:\n",
    "            print(f\"No file found. Looking at {self.savefolder}.\")\n",
    "\n",
    "    @property\n",
    "    def blotch_file(self):\n",
    "        \"Return path to blotch catalog file.\"\n",
    "        try:\n",
    "            return next(self.savefolder.glob(\"*_blotch.csv\"))\n",
    "        except StopIteration:\n",
    "            print(f\"No file found. Looking at {self.savefolder}.\")\n",
    "\n",
    "    @property\n",
    "    def fan_merged(self):\n",
    "        return self.fan_file.parent / f\"{self.fan_file.stem}_meta_merged.csv\"\n",
    "\n",
    "    @property\n",
    "    def blotch_merged(self):\n",
    "        return self.blotch_file.parent / f\"{self.blotch_file.stem}_meta_merged.csv\"\n",
    "\n",
    "    def read_fan_file(self):\n",
    "        return pd.read_csv(self.fan_merged)\n",
    "\n",
    "    def read_blotch_file(self):\n",
    "        return pd.read_csv(self.blotch_merged)\n",
    "\n",
    "    def check_for_todo(self, overwrite=None):\n",
    "        if overwrite is None:\n",
    "            overwrite = self.overwrite\n",
    "        bucket = []\n",
    "        for obsid in self.obsids:\n",
    "            pm = io.PathManager(obsid=obsid, datapath=self.savefolder)\n",
    "            path = pm.obsid_results_savefolder / obsid\n",
    "            if path.exists() and overwrite is False:\n",
    "                continue\n",
    "            else:\n",
    "                bucket.append(obsid)\n",
    "        self.todo = bucket\n",
    "\n",
    "    def get_parallel_args(self):\n",
    "        return [(i, self.catalog, self.dbname) for i in self.todo]\n",
    "\n",
    "    def get_no_of_tiles_per_obsid(self):\n",
    "        all_data = pd.read_parquet(self.dbname)\n",
    "        return all_data.groupby(\"image_name\").image_id.nunique()\n",
    "\n",
    "    @property\n",
    "    def EDRINDEX_meta_path(self):\n",
    "        return self.savefolder / f\"{self.catalog}_EDRINDEX_metadata.csv\"\n",
    "\n",
    "    def calc_metadata(self):\n",
    "        if not self.EDRINDEX_meta_path.exists():\n",
    "            NAs = p4meta.get_north_azimuths_from_SPICE(self.obsids)\n",
    "            edrindex = get_index(\"mro.hirise\", \"edr\")\n",
    "            p4_edr = (\n",
    "                edrindex[edrindex.OBSERVATION_ID.isin(self.obsids)]\n",
    "                .query('CCD_NAME==\"RED4\"')\n",
    "                .drop_duplicates(subset=\"OBSERVATION_ID\")\n",
    "            )\n",
    "            p4_edr = p4_edr.set_index(\"OBSERVATION_ID\").join(\n",
    "                NAs.set_index(\"OBSERVATION_ID\")\n",
    "            )\n",
    "            p4_edr = p4_edr.join(self.get_no_of_tiles_per_obsid())\n",
    "            p4_edr.rename(dict(image_id=\"# of tiles\"), axis=1, inplace=True)\n",
    "            p4_edr[\"map_scale\"] = 0.25 * p4_edr.BINNING\n",
    "            p4_edr.reset_index(inplace=True)\n",
    "            p4_edr.to_csv(self.EDRINDEX_meta_path)\n",
    "        else:\n",
    "            p4_edr = pd.read_csv(self.EDRINDEX_meta_path)\n",
    "        cols = [\n",
    "            \"OBSERVATION_ID\",\n",
    "            \"IMAGE_CENTER_LATITUDE\",\n",
    "            \"IMAGE_CENTER_LONGITUDE\",\n",
    "            \"SOLAR_LONGITUDE\",\n",
    "            \"START_TIME\",\n",
    "            \"map_scale\",\n",
    "            \"north_azimuth\",\n",
    "            \"# of tiles\",\n",
    "        ]\n",
    "        metadata = p4_edr[cols]\n",
    "        metadata.to_csv(self.metadata_path, index=False, float_format=\"%.7f\")\n",
    "        LOGGER.info(\"Wrote %s\", str(self.metadata_path))\n",
    "\n",
    "    def calc_tile_coordinates(self):\n",
    "        cubepaths = [P4Mosaic(obsid).mosaic_path for obsid in self.obsids]\n",
    "\n",
    "        todo = []\n",
    "        for cubepath in cubepaths:\n",
    "            tc = TileCalculator(cubepath, read_data=False, dbname=self.dbname)\n",
    "            if not tc.campt_results_path.exists():\n",
    "                todo.append(cubepath)\n",
    "\n",
    "        def get_tile_coords(cubepath):\n",
    "            from planet4.projection import TileCalculator##TODO is that import necessary\n",
    "\n",
    "            tilecalc = TileCalculator(cubepath, dbname=self.dbname)\n",
    "            tilecalc.calc_tile_coords()\n",
    "\n",
    "        if not len(todo) == 0:\n",
    "            _ = execute_in_parallel(get_tile_coords, todo)##Execute in parallel?\n",
    "\n",
    "        bucket = []\n",
    "        for cubepath in tqdm(cubepaths):\n",
    "            tc = TileCalculator(cubepath, read_data=False, dbname=self.dbname)\n",
    "            bucket.append(tc.tile_coords_df)\n",
    "        coords = pd.concat(bucket, ignore_index=True, sort=False)\n",
    "        coords.to_csv(self.tile_coords_path, index=False, float_format=\"%.7f\")\n",
    "        LOGGER.info(\"Wrote %s\", str(self.tile_coords_path))\n",
    "\n",
    "    @property\n",
    "    def COLS_TO_MERGE(self):\n",
    "        return [\n",
    "            \"obsid\",\n",
    "            \"image_x\",\n",
    "            \"image_y\",\n",
    "            \"BodyFixedCoordinateX\",\n",
    "            \"BodyFixedCoordinateY\",\n",
    "            \"BodyFixedCoordinateZ\",\n",
    "            \"PlanetocentricLatitude\",\n",
    "            \"PlanetographicLatitude\",\n",
    "            \"PositiveEast360Longitude\",\n",
    "        ]\n",
    "\n",
    "    def merge_fnotch_results(self, fans, blotches):\n",
    "        \"\"\"Average multiple objects from fnotching into one.\n",
    "\n",
    "        Because fnotching can compare the same object with more than one, it can appear more than once\n",
    "        with different `vote_ratio` values in the results. We merge them here into one, simply\n",
    "        averaging the vote_ratio. This increases the value of the `vote_ratio` number as it now\n",
    "        has been created by several comparisons. It only occurs for 0.5 % of fans though.\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for df in [fans, blotches]:\n",
    "            averaged = df.groupby(\"marking_id\").mean(numeric_only=True)\n",
    "            tmp = df.drop_duplicates(subset=\"marking_id\").set_index(\"marking_id\")\n",
    "            averaged = averaged.join(tmp[[\"image_id\", \"obsid\"]], how=\"inner\")\n",
    "            out.append(averaged.reset_index())\n",
    "\n",
    "        return out\n",
    "\n",
    "    def merge_all(self):\n",
    "        # read in data files\n",
    "        fans = pd.read_csv(self.fan_file)\n",
    "        blotches = pd.read_csv(self.blotch_file)\n",
    "        meta = pd.read_csv(self.metadata_path, dtype=\"str\")\n",
    "        tile_coords = pd.read_csv(self.tile_coords_path, dtype=\"str\")\n",
    "\n",
    "        # average multiple fnotch results\n",
    "        fans, blotches = self.merge_fnotch_results(fans, blotches)\n",
    "\n",
    "        # merge meta\n",
    "        cols_to_merge = [\n",
    "            \"OBSERVATION_ID\",\n",
    "            \"SOLAR_LONGITUDE\",\n",
    "            \"north_azimuth\",\n",
    "            \"map_scale\",\n",
    "        ]\n",
    "        fans = fans.merge(\n",
    "            meta[cols_to_merge], left_on=\"obsid\", right_on=\"OBSERVATION_ID\"\n",
    "        )\n",
    "        blotches = blotches.merge(\n",
    "            meta[cols_to_merge], left_on=\"obsid\", right_on=\"OBSERVATION_ID\"\n",
    "        )\n",
    "\n",
    "        # drop unnecessary columns\n",
    "        tile_coords.drop(\n",
    "            self.DROP_FOR_TILE_COORDS, axis=1, inplace=True, errors=\"ignore\"\n",
    "        )\n",
    "        # save cleaned tile_coords\n",
    "        tile_coords.rename({\"image_id\": \"tile_id\"}, axis=1, inplace=True)\n",
    "        tile_coords.to_csv(\n",
    "            self.tile_coords_path_final, index=False, float_format=\"%.7f\"\n",
    "        )\n",
    "\n",
    "        # merge campt results into catalog files\n",
    "        fans, blotches = self.merge_campt_results(fans, blotches)\n",
    "\n",
    "        # write out fans catalog\n",
    "        fans.vote_ratio.fillna(1, inplace=True)\n",
    "        fans.version = fans.version.astype(\"int\")\n",
    "        fans.rename(\n",
    "            {\n",
    "                \"image_id\": \"tile_id\",\n",
    "                \"SOLAR_LONGITUDE\": \"l_s\",\n",
    "                \"PositiveEast360Longitude\": \"Longitude\",\n",
    "            },\n",
    "            axis=1,\n",
    "            inplace=True,\n",
    "        )\n",
    "        fans[self.FAN_COLUMNS_AS_PUBLISHED].to_csv(self.fan_merged, index=False)\n",
    "        LOGGER.info(\"Wrote %s\", str(self.fan_merged))\n",
    "\n",
    "        # write out blotches catalog\n",
    "        blotches.vote_ratio.fillna(1, inplace=True)\n",
    "        blotches.rename(\n",
    "            {\n",
    "                \"image_id\": \"tile_id\",\n",
    "                \"SOLAR_LONGITUDE\": \"l_s\",\n",
    "                \"PositiveEast360Longitude\": \"Longitude\",\n",
    "            },\n",
    "            axis=1,\n",
    "            inplace=True,\n",
    "        )\n",
    "        blotches[self.BLOTCH_COLUMNS_AS_PUBLISHED].to_csv(\n",
    "            self.blotch_merged, index=False\n",
    "        )\n",
    "        LOGGER.info(\"Wrote %s\", str(self.blotch_merged))\n",
    "\n",
    "    def calc_marking_coordinates(self):\n",
    "        fans = pd.read_csv(self.fan_file)\n",
    "        blotches = pd.read_csv(self.blotch_file)\n",
    "        combined = pd.concat([fans, blotches], sort=False)\n",
    "\n",
    "        for obsid in tqdm(self.obsids):\n",
    "            data = combined[combined.image_name == obsid]\n",
    "            xy = XY2LATLON(data, self.savefolder, overwrite=self.overwrite)\n",
    "            xy.process_inpath()\n",
    "\n",
    "    def collect_marking_coordinates(self):\n",
    "        bucket = []\n",
    "        for obsid in self.obsids:\n",
    "            xy = XY2LATLON(None, self.savefolder, obsid=obsid)\n",
    "            bucket.append(pd.read_csv(xy.savepath).assign(obsid=obsid))\n",
    "\n",
    "        ground = pd.concat(bucket, sort=False).drop_duplicates()\n",
    "        ground.rename(dict(Sample=\"image_x\", Line=\"image_y\"), axis=1, inplace=True)\n",
    "        return ground\n",
    "\n",
    "    def fix_marking_coordinates_precision(self, df):\n",
    "        fname = \"tempfile.csv\"\n",
    "        df.to_csv(fname, float_format=\"%.7f\")\n",
    "        return pd.read_csv(fname, dtype=\"str\")\n",
    "\n",
    "    def merge_campt_results(self, fans, blotches):\n",
    "        INDEX = [\"obsid\", \"image_x\", \"image_y\"]\n",
    "\n",
    "        ground = self.collect_marking_coordinates().round(decimals=7)\n",
    "        # ground = self.fix_marking_coordinates_precision(ground)\n",
    "        fans = fans.merge(ground[self.COLS_TO_MERGE], on=INDEX)\n",
    "        blotches = blotches.merge(ground[self.COLS_TO_MERGE], on=INDEX)\n",
    "        return fans, blotches\n",
    "\n",
    "    def perform_clustering(self):\n",
    "        lazy_results = []\n",
    "\n",
    "    def launch_catalog_production(self):\n",
    "        # check for data that is unprocessed\n",
    "        self.check_for_todo()\n",
    "\n",
    "        # perform the clustering\n",
    "        if len(self.todo) > 0:\n",
    "            LOGGER.info(\"Performing the clustering.\")\n",
    "            results = cluster_obsid_parallel(self.todo, self.catalog, self.dbname)\n",
    "\n",
    "            # create marking_ids\n",
    "            fan_id = fan_id_generator()\n",
    "            blotch_id = blotch_id_generator()\n",
    "            for obsid in self.todo:\n",
    "                paths = get_L1A_paths(obsid, self.catalog)\n",
    "                for path in paths:\n",
    "                    add_marking_ids(path, fan_id, blotch_id)\n",
    "\n",
    "            # fnotch and apply cuts\n",
    "            LOGGER.info(\"Start fnotching\")\n",
    "            results = fnotch_obsid_parallel(self.todo, self.catalog)\n",
    "\n",
    "        # create summary CSV files of the clustering output\n",
    "        LOGGER.info(\"Creating L1C fan and blotch database files.\")\n",
    "        create_roi_file(self.obsids, self.catalog, self.catalog)\n",
    "\n",
    "        LOGGER.info(\"Creating the required RED45 mosaics for ground projections.\")\n",
    "        results = execute_in_parallel(create_RED45_mosaic, self.obsids)\n",
    "\n",
    "        LOGGER.info(\"Calculating the center ground coordinates for all P4 tiles.\")\n",
    "        self.calc_tile_coordinates()\n",
    "\n",
    "        LOGGER.info(\"Calculating ground coordinates for catalog.\")\n",
    "        self.calc_marking_coordinates()\n",
    "\n",
    "        # calculate all metadata required for P4 analysis\n",
    "        LOGGER.info(\"Writing summary metadata file.\")\n",
    "        self.calc_metadata()\n",
    "        # merging metadata\n",
    "        self.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def fan_id_generator():\n",
    "    for newid in itertools.product(string.digits + \"abcdef\", repeat=6):\n",
    "        yield \"F\" + \"\".join(newid)\n",
    "\n",
    "\n",
    "def blotch_id_generator():\n",
    "    for newid in itertools.product(string.digits + \"abcdef\", repeat=6):\n",
    "        yield \"B\" + \"\".join(newid)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
