{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp production.catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# production.catalog\n",
    "\n",
    "> this submodule stores the main functions to create a catalog from the planet4 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# other imports\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import logging\n",
    "import itertools\n",
    "from planetarypy.pds.apps import get_index\n",
    "import string\n",
    "from dask import delayed, compute\n",
    "import numpy as np\n",
    "\n",
    "# p4tools package imports\n",
    "import p4tools.production.io as io\n",
    "import p4tools.production.metadata as p4meta\n",
    "from p4tools.production.projection import XY2LATLON, P4Mosaic, TileCalculator, create_RED45_mosaic\n",
    "\n",
    "\n",
    "#typing imports\n",
    "from collections.abc import Iterable,Callable\n",
    "from typing import Any, Generator\n",
    "from logging import Logger\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export \n",
    "\n",
    "#starting the logger\n",
    "LOGGER: Logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export \n",
    "\n",
    "def execute_in_parallel(func : Callable, iterable : Iterable):\n",
    "    \"\"\"This function is used to execute a function in paralle over a list-like or iterable using the power of Dask's lazy compute.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : Callable \n",
    "        The function that is to be parallel computed over the iterable does not need to accept a\n",
    "    iterable : Iterable\n",
    "        The Iterable over which we want to execute the function for each of the elements in the iterable \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List\n",
    "        The results of the function reduced over the Iterable\n",
    "    \"\"\"\n",
    "    lazys = []\n",
    "    for item in iterable:\n",
    "        lazys.append(delayed(func)(item))\n",
    "    return compute(*lazys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "from typing import Any, Generator\n",
    "\n",
    "\n",
    "def fan_id_generator() -> Generator[str, Any, None]:\n",
    "    \"\"\"Generatir for the IDs to number the fans\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    Generator[str, Any, None]\n",
    "        ID generator\n",
    "    \"\"\"\n",
    "    for newid in itertools.product(string.digits + \"abcdef\", repeat=6):\n",
    "        yield \"F\" + \"\".join(newid)\n",
    "\n",
    "\n",
    "def blotch_id_generator() -> Generator[str, Any, None]:\n",
    "    \"\"\"Generator to yield the IDs to number the blotches\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    Generator[str, Any, None]\n",
    "        ID generator\n",
    "    \"\"\"\n",
    "    for newid in itertools.product(string.digits + \"abcdef\", repeat=6):\n",
    "        yield \"B\" + \"\".join(newid)\n",
    "\n",
    "\n",
    "def get_L1A_paths(obsid, savefolder):\n",
    "    pm = io.PathManager(obsid=obsid, datapath=savefolder)\n",
    "    paths = pm.get_obsid_paths(\"L1A\")\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def cluster_obsid(obsid=None, savedir=None, imgid=None, dbname=None):\n",
    "    \"\"\"Cluster all image_ids for given obsid (=image_name).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obsid : str\n",
    "        HiRISE obsid (= Planet four image_name)\n",
    "    savedir : str or pathlib.Path\n",
    "        Top directory path where the catalog will be stored. Will create folder if it\n",
    "        does not exist yet.\n",
    "    imgid : str, optional\n",
    "        Convenience parameter: If `obsid` is not given and therefore is None, this `image_id` can\n",
    "        be used to receive the respective `obsid` from the TileID class.\n",
    "    \"\"\"\n",
    "    # import here to support parallel execution\n",
    "    from p4tools.production import markings, dbscan\n",
    "    \n",
    "    # parameter checks\n",
    "    if obsid is None and imgid is not None:\n",
    "        obsid = markings.TileID(imgid).image_name\n",
    "    elif obsid is None and imgid is None:\n",
    "        raise ValueError(\"Provide either obsid or imgid.\")\n",
    "\n",
    "    # cluster\n",
    "    dbscanner = dbscan.DBScanner(savedir=savedir, dbname=dbname)\n",
    "    dbscanner.cluster_image_name(obsid)\n",
    "    return obsid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def fnotch_obsid(obsid=None, savedir=None, fnotch_via_obsid=False, imgid=None):\n",
    "    \"\"\"\n",
    "    fnotch_via_obsid: bool, optional\n",
    "        Switch to control if fnotching happens per image_id or per obsid\n",
    "    \"\"\"\n",
    "    from p4tools.production import fnotching\n",
    "\n",
    "    # fnotching / combining ambiguous cluster results\n",
    "    # fnotch across all the HiRISE image\n",
    "    # does not work yet correctly! Needs to scale for n_classifications\n",
    "    if fnotch_via_obsid is True:\n",
    "        fnotching.fnotch_obsid(obsid, savedir=savedir)\n",
    "        fnotching.apply_cut_obsid(obsid, savedir=savedir)\n",
    "    else:\n",
    "        # default case: Fnotch for each image_id separately.\n",
    "        fnotching.fnotch_image_ids(obsid, savedir=savedir)\n",
    "        fnotching.apply_cut(obsid, savedir=savedir)\n",
    "    return obsid\n",
    "\n",
    "\n",
    "def fnotch_obsid_parallel(obsids : list[str], savedir : str):\n",
    "    \"\"\"Applies the fnotching for multiple obsid's in parallel\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obsids : list[str]\n",
    "        List of the Obsids to fnotch\n",
    "    savedir : str\n",
    "        the directory path where to save\n",
    "    \"\"\"\n",
    "    lazys = []\n",
    "    for obsid in obsids:\n",
    "        lazys.append(delayed(fnotch_obsid)(obsid, savedir))\n",
    "    return compute(*lazys)\n",
    "\n",
    "\n",
    "def cluster_obsid_parallel(obsids : list[str], savedir : str, dbname : str):\n",
    "    \"\"\"Apply the Clustering Algorithm for multiple obsids in parallel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obsids : list[str]\n",
    "        List of the obsids to cluster\n",
    "    savedir : str\n",
    "        path to the save directory whihc will save the clustering results\n",
    "    dbname : str\n",
    "        The databasename \n",
    "    \"\"\"\n",
    "    lazys = []\n",
    "    for obsid in obsids:\n",
    "        lazys.append(delayed(cluster_obsid)(obsid, savedir, dbname=dbname))\n",
    "    return compute(*lazys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def add_marking_ids(path, fan_id, blotch_id):\n",
    "    \"\"\"Add marking_ids for catalog to cluster results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str, pathlib.Path\n",
    "        Path to L1A image_id clustering result directory\n",
    "    fan_id, blotch_id : generator\n",
    "        Generator for marking_id\n",
    "    \"\"\"\n",
    "    image_id = path.parent.name\n",
    "    for kind, id_ in zip([\"fans\", \"blotches\"], [fan_id, blotch_id]):\n",
    "        fname = str(path / f\"{image_id}_L1A_{kind}.csv\")\n",
    "        try:\n",
    "            df = pd.read_csv(fname)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        else:\n",
    "            marking_ids = []\n",
    "            for _ in range(df.shape[0]):\n",
    "                marking_ids.append(next(id_))\n",
    "            df[\"marking_id\"] = marking_ids\n",
    "            df.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export \n",
    "\n",
    "def create_roi_file(obsids, roi_name, datapath):\n",
    "    \"\"\"Create a Region of Interest file, based on list of obsids.\n",
    "\n",
    "    For more structured analysis processes, we can create a summary file for a list of obsids\n",
    "    belonging to a ROI.\n",
    "    The alternative is to define to what ROI any final object belongs to and add that as a column\n",
    "    in the final catalog.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obsids : iterable of str\n",
    "        List of HiRISE obsids\n",
    "    roi_name : str\n",
    "        Name for ROI\n",
    "    datapath : str or pathlib.Path\n",
    "        Path to the top folder with the clustering output data.\n",
    "    \"\"\"\n",
    "    Bucket = dict(fan=[], blotch=[])\n",
    "    for obsid in tqdm(obsids):\n",
    "        pm = io.PathManager(obsid=obsid, datapath=datapath)\n",
    "        # get all L1C folders for current obsid:\n",
    "        folders = pm.get_obsid_paths(\"L1C\")\n",
    "        bucket = read_csvfiles_into_lists_of_frames(folders)\n",
    "        for key, val in bucket.items():\n",
    "            try:\n",
    "                df = pd.concat(val, ignore_index=True, sort=False)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            else:\n",
    "                df[\"obsid\"] = obsid\n",
    "                Bucket[key].append(df)\n",
    "    savedir = pm.path_so_far.parent\n",
    "    if len(Bucket) == 0:\n",
    "        func = LOGGER.warning\n",
    "    else:\n",
    "        func = LOGGER.info\n",
    "    func(\"Found %i fans and %i blotches.\", len(Bucket[\"fan\"]), len(Bucket[\"blotch\"]))\n",
    "    for key, val in Bucket.items():\n",
    "        try:\n",
    "            df = pd.concat(val, ignore_index=True, sort=False)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        else:\n",
    "            savename = f\"{roi_name}_{pm.L1C_folder}_{key}.csv\"\n",
    "            savepath = savedir / savename\n",
    "            for col in [\"x_tile\", \"y_tile\"]:\n",
    "                df[col] = pd.to_numeric(df[col], downcast=\"signed\")\n",
    "            if \"version\" in df.columns:\n",
    "                df[\"version\"] = pd.to_numeric(df[\"version\"], downcast=\"signed\")\n",
    "            df.to_csv(savepath, index=False, float_format=\"%.2f\")\n",
    "            print(f\"Created {savepath}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ReleaseManager:\n",
    "    \"\"\"Class to manage releases and find relevant files.\n",
    "    TODO better description\n",
    "    Parameters\n",
    "    ----------\n",
    "    version : str\n",
    "        Version string for this catalog. Same as datapath in other P4 code.\n",
    "    obsids : iterable, optional\n",
    "        Iterable of obsids that should be used for catalog file. Default is to use the full list of the default database, which is Seasons 2 and 3 at this point.\n",
    "    overwrite : bool, optional\n",
    "        Switch to control if already existing result folders for an obsid should be overwritten.\n",
    "        Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    DROP_FOR_TILE_COORDS: list[str] = [\n",
    "        \"xy_hirise\",\n",
    "        \"SampleResolution\",\n",
    "        \"LineResolution\",\n",
    "        \"PositiveWest360Longitude\",\n",
    "        \"Line\",\n",
    "        \"Sample\",\n",
    "    ]\n",
    "\n",
    "    FAN_COLUMNS_AS_PUBLISHED: list[str] = [\n",
    "        \"marking_id\",\n",
    "        \"angle\",\n",
    "        \"distance\",\n",
    "        \"tile_id\",\n",
    "        \"image_x\",\n",
    "        \"image_y\",\n",
    "        \"n_votes\",\n",
    "        \"obsid\",\n",
    "        \"spread\",\n",
    "        \"version\",\n",
    "        \"vote_ratio\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"x_angle\",\n",
    "        \"y_angle\",\n",
    "        \"l_s\",\n",
    "        \"map_scale\",\n",
    "        \"north_azimuth\",\n",
    "        \"BodyFixedCoordinateX\",\n",
    "        \"BodyFixedCoordinateY\",\n",
    "        \"BodyFixedCoordinateZ\",\n",
    "        \"PlanetocentricLatitude\",\n",
    "        \"PlanetographicLatitude\",\n",
    "        \"Longitude\",\n",
    "    ]\n",
    "    BLOTCH_COLUMNS_AS_PUBLISHED: list[str] = [\n",
    "        \"marking_id\",\n",
    "        \"angle\",\n",
    "        \"tile_id\",\n",
    "        \"image_x\",\n",
    "        \"image_y\",\n",
    "        \"n_votes\",\n",
    "        \"obsid\",\n",
    "        \"radius_1\",\n",
    "        \"radius_2\",\n",
    "        \"vote_ratio\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"x_angle\",\n",
    "        \"y_angle\",\n",
    "        \"l_s\",\n",
    "        \"map_scale\",\n",
    "        \"north_azimuth\",\n",
    "        \"BodyFixedCoordinateX\",\n",
    "        \"BodyFixedCoordinateY\",\n",
    "        \"BodyFixedCoordinateZ\",\n",
    "        \"PlanetocentricLatitude\",\n",
    "        \"PlanetographicLatitude\",\n",
    "        \"Longitude\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, version, obsids=None, overwrite=False, dbname=None):\n",
    "        self.catalog = f\"P4_catalog_{version}\"\n",
    "        self.overwrite = overwrite\n",
    "        self._obsids: Iterable | None = obsids\n",
    "        self.dbname = dbname\n",
    "\n",
    "    @property\n",
    "    def savefolder(self):\n",
    "        \"Path to catalog folder\"\n",
    "        return io.data_root / self.catalog\n",
    "\n",
    "    @property\n",
    "    def metadata_path(self):\n",
    "        \"Path to catalog metadata file.\"\n",
    "        return self.savefolder / f\"{self.catalog}_metadata.csv\"\n",
    "\n",
    "    @property\n",
    "    def tile_coords_path(self):\n",
    "        \"Path to catalog tile coordinates file.\"\n",
    "        return self.savefolder / f\"{self.catalog}_tile_coords.csv\"\n",
    "\n",
    "    @property\n",
    "    def tile_coords_path_final(self):\n",
    "        \"Path to final catalog tile coordinates file.\"\n",
    "        return self.savefolder / f\"{self.catalog}_tile_coords_final.csv\"\n",
    "\n",
    "    @property\n",
    "    def obsids(self):\n",
    "        \"\"\"Return list of obsids for catalog production.\n",
    "\n",
    "        If ._obsids is None, get default full obsids list for current default P4 database.\n",
    "        \"\"\"\n",
    "        if self._obsids is None:\n",
    "            db = io.DBManager(dbname=self.dbname)\n",
    "            self._obsids = db.obsids\n",
    "        return self._obsids\n",
    "\n",
    "    @obsids.setter\n",
    "    def obsids(self, values):\n",
    "        self._obsids = values\n",
    "\n",
    "    @property\n",
    "    def fan_file(self):\n",
    "        \"Return path to fan catalog file.\"\n",
    "        try:\n",
    "            return next(self.savefolder.glob(\"*_fan.csv\"))\n",
    "        except StopIteration:\n",
    "            print(f\"No file found. Looking at {self.savefolder}.\")\n",
    "\n",
    "    @property\n",
    "    def blotch_file(self):\n",
    "        \"Return path to blotch catalog file.\"\n",
    "        try:\n",
    "            return next(self.savefolder.glob(\"*_blotch.csv\"))\n",
    "        except StopIteration:\n",
    "            print(f\"No file found. Looking at {self.savefolder}.\")\n",
    "\n",
    "    @property\n",
    "    def fan_merged(self):\n",
    "        return self.fan_file.parent / f\"{self.fan_file.stem}_meta_merged.csv\"\n",
    "\n",
    "    @property\n",
    "    def blotch_merged(self):\n",
    "        return self.blotch_file.parent / f\"{self.blotch_file.stem}_meta_merged.csv\"\n",
    "\n",
    "    def read_fan_file(self):\n",
    "        return pd.read_csv(self.fan_merged)\n",
    "\n",
    "    def read_blotch_file(self):\n",
    "        return pd.read_csv(self.blotch_merged)\n",
    "\n",
    "    def mark_done(self,obsid):\n",
    "        \"\"\"Create a simple file in each obsid folder, that is simply meant to show that this obsid was finished for the todo method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        obsid : str\n",
    "            Corresponding obsid\n",
    "        \"\"\"\n",
    "        pm = io.PathManager(obsid=obsid, datapath=self.savefolder)\n",
    "        path = pm.obsid_results_savefolder / obsid / \"Done.txt\"\n",
    "        with open(path, \"w\") as file:\n",
    "            file.write(\"Done\")\n",
    "    \n",
    "\n",
    "    def check_for_todo(self, overwrite=None):\n",
    "        if overwrite is None:\n",
    "            overwrite = self.overwrite\n",
    "        bucket = []\n",
    "        for obsid in self.obsids:\n",
    "            pm = io.PathManager(obsid=obsid, datapath=self.savefolder)\n",
    "            path = pm.obsid_results_savefolder / obsid / \"Done.txt\"\n",
    "            if path.exists() and overwrite is False:\n",
    "                continue\n",
    "            else:\n",
    "                bucket.append(obsid)\n",
    "        self.todo = bucket\n",
    "\n",
    "    def get_parallel_args(self):\n",
    "        return [(i, self.catalog, self.dbname) for i in self.todo]\n",
    "\n",
    "    def get_no_of_tiles_per_obsid(self):\n",
    "        all_data = pd.read_parquet(self.dbname)\n",
    "        return all_data.groupby(\"image_name\").image_id.nunique()\n",
    "\n",
    "    @property\n",
    "    def EDRINDEX_meta_path(self):\n",
    "        return self.savefolder / f\"{self.catalog}_EDRINDEX_metadata.csv\"\n",
    "\n",
    "    def calc_metadata(self):\n",
    "        if not self.EDRINDEX_meta_path.exists():\n",
    "            NAs = p4meta.get_north_azimuths_from_SPICE(self.obsids)\n",
    "            edrindex = get_index(\"mro.hirise\", \"edr\")\n",
    "            p4_edr = (\n",
    "                edrindex[edrindex.OBSERVATION_ID.isin(self.obsids)]\n",
    "                .query('CCD_NAME==\"RED4\"')\n",
    "                .drop_duplicates(subset=\"OBSERVATION_ID\")\n",
    "            )\n",
    "            p4_edr = p4_edr.set_index(\"OBSERVATION_ID\").join(\n",
    "                NAs.set_index(\"OBSERVATION_ID\")\n",
    "            )\n",
    "            p4_edr = p4_edr.join(self.get_no_of_tiles_per_obsid())\n",
    "            p4_edr.rename(dict(image_id=\"# of tiles\"), axis=1, inplace=True)\n",
    "            p4_edr[\"map_scale\"] = 0.25 * p4_edr.BINNING\n",
    "            p4_edr.reset_index(inplace=True)\n",
    "            p4_edr.to_csv(self.EDRINDEX_meta_path)\n",
    "        else:\n",
    "            p4_edr = pd.read_csv(self.EDRINDEX_meta_path)\n",
    "        cols = [\n",
    "            \"OBSERVATION_ID\",\n",
    "            \"IMAGE_CENTER_LATITUDE\",\n",
    "            \"IMAGE_CENTER_LONGITUDE\",\n",
    "            \"SOLAR_LONGITUDE\",\n",
    "            \"START_TIME\",\n",
    "            \"map_scale\",\n",
    "            \"north_azimuth\",\n",
    "            \"# of tiles\",\n",
    "        ]\n",
    "        metadata = p4_edr[cols]\n",
    "        metadata.to_csv(self.metadata_path, index=False, float_format=\"%.7f\")\n",
    "        LOGGER.info(\"Wrote %s\", str(self.metadata_path))\n",
    "\n",
    "    def calc_tile_coordinates(self):\n",
    "        cubepaths = [P4Mosaic(obsid).mosaic_path for obsid in self.obsids]\n",
    "\n",
    "        todo = []\n",
    "        for cubepath in tqdm(cubepaths,desc=\"Appending Cubepaths\"):\n",
    "            tc = TileCalculator(cubepath, read_data=False, dbname=self.dbname)\n",
    "            if not tc.campt_results_path.exists():\n",
    "                todo.append(cubepath)\n",
    "\n",
    "        def get_tile_coords(cubepath):\n",
    "            from p4tools.production.projection import TileCalculator##TODO is that import necessary\n",
    "\n",
    "            tilecalc = TileCalculator(cubepath, dbname=self.dbname)\n",
    "            tilecalc.calc_tile_coords()\n",
    "\n",
    "        if not len(todo) == 0:\n",
    "            for cubepath in tqdm(todo,desc=\"Calculating Tile Coords\"):\n",
    "                _ = get_tile_coords(cubepath)\n",
    "\n",
    "        bucket = []\n",
    "        for cubepath in tqdm(cubepaths,desc=\"Creating Cubepath Bucket\"):\n",
    "            tc = TileCalculator(cubepath, read_data=False, dbname=self.dbname)\n",
    "            bucket.append(tc.tile_coords_df)\n",
    "        coords = pd.concat(bucket, ignore_index=True, sort=False)\n",
    "        coords.to_csv(self.tile_coords_path, index=False, float_format=\"%.7f\")\n",
    "        LOGGER.info(\"Wrote %s\", str(self.tile_coords_path))\n",
    "\n",
    "    @property\n",
    "    def COLS_TO_MERGE(self):\n",
    "        return [\n",
    "            \"obsid\",\n",
    "            \"image_x\",\n",
    "            \"image_y\",\n",
    "            \"BodyFixedCoordinateX\",\n",
    "            \"BodyFixedCoordinateY\",\n",
    "            \"BodyFixedCoordinateZ\",\n",
    "            \"PlanetocentricLatitude\",\n",
    "            \"PlanetographicLatitude\",\n",
    "            \"PositiveEast360Longitude\",\n",
    "        ]\n",
    "\n",
    "    def merge_fnotch_results(self, fans, blotches):\n",
    "        \"\"\"Average multiple objects from fnotching into one.\n",
    "\n",
    "        Because fnotching can compare the same object with more than one, it can appear more than once\n",
    "        with different `vote_ratio` values in the results. We merge them here into one, simply\n",
    "        averaging the vote_ratio. This increases the value of the `vote_ratio` number as it now\n",
    "        has been created by several comparisons. It only occurs for 0.5 % of fans though.\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for df in [fans, blotches]:\n",
    "            averaged = df.groupby(\"marking_id\").mean(numeric_only=True)\n",
    "            tmp = df.drop_duplicates(subset=\"marking_id\").set_index(\"marking_id\")\n",
    "            averaged = averaged.join(tmp[[\"image_id\", \"obsid\"]], how=\"inner\")\n",
    "            out.append(averaged.reset_index())\n",
    "\n",
    "        return out\n",
    "\n",
    "    def merge_all(self):\n",
    "        # read in data files\n",
    "        fans = pd.read_csv(self.fan_file)\n",
    "        blotches = pd.read_csv(self.blotch_file)\n",
    "        meta = pd.read_csv(self.metadata_path, dtype=\"str\")\n",
    "        tile_coords = pd.read_csv(self.tile_coords_path, dtype=\"str\")\n",
    "\n",
    "        # average multiple fnotch results\n",
    "        fans, blotches = self.merge_fnotch_results(fans, blotches)\n",
    "\n",
    "        # merge meta\n",
    "        cols_to_merge = [\n",
    "            \"OBSERVATION_ID\",\n",
    "            \"SOLAR_LONGITUDE\",\n",
    "            \"north_azimuth\",\n",
    "            \"map_scale\",\n",
    "        ]\n",
    "        fans = fans.merge(\n",
    "            meta[cols_to_merge], left_on=\"obsid\", right_on=\"OBSERVATION_ID\"\n",
    "        )\n",
    "        blotches = blotches.merge(\n",
    "            meta[cols_to_merge], left_on=\"obsid\", right_on=\"OBSERVATION_ID\"\n",
    "        )\n",
    "\n",
    "        # drop unnecessary columns\n",
    "        tile_coords.drop(\n",
    "            self.DROP_FOR_TILE_COORDS, axis=1, inplace=True, errors=\"ignore\"\n",
    "        )\n",
    "        # save cleaned tile_coords\n",
    "        tile_coords.rename({\"image_id\": \"tile_id\"}, axis=1, inplace=True)\n",
    "        tile_coords.to_csv(\n",
    "            self.tile_coords_path_final, index=False, float_format=\"%.7f\"\n",
    "        )\n",
    "\n",
    "        # merge campt results into catalog files\n",
    "        fans, blotches = self.merge_campt_results(fans, blotches)\n",
    "\n",
    "        # write out fans catalog\n",
    "        fans[\"vote_ratio\"] = fans[\"vote_ratio\"].fillna(1)\n",
    "        fans.version = fans.version.astype(\"int\")\n",
    "        fans.rename(\n",
    "            {\n",
    "                \"image_id\": \"tile_id\",\n",
    "                \"SOLAR_LONGITUDE\": \"l_s\",\n",
    "                \"PositiveEast360Longitude\": \"Longitude\",\n",
    "            },\n",
    "            axis=1,\n",
    "            inplace=True,\n",
    "        )\n",
    "        fans[self.FAN_COLUMNS_AS_PUBLISHED].to_csv(self.fan_merged, index=False, mode = \"a\")\n",
    "\n",
    "        LOGGER.info(\"Wrote %s\", str(self.fan_merged))\n",
    "\n",
    "        # write out blotches catalog\n",
    "        blotches[\"vote_ratio\"] = blotches[\"vote_ratio\"].fillna(1)\n",
    "        blotches.rename(\n",
    "            {\n",
    "                \"image_id\": \"tile_id\",\n",
    "                \"SOLAR_LONGITUDE\": \"l_s\",\n",
    "                \"PositiveEast360Longitude\": \"Longitude\",\n",
    "            },\n",
    "            axis=1,\n",
    "            inplace=True,\n",
    "        )\n",
    "        blotches[self.BLOTCH_COLUMNS_AS_PUBLISHED].to_csv(\n",
    "            self.blotch_merged, index=False, mode = \"a\"\n",
    "        )\n",
    "        LOGGER.info(\"Wrote %s\", str(self.blotch_merged))\n",
    "\n",
    "    def calc_marking_coordinates(self):\n",
    "        fans = pd.read_csv(self.fan_file)\n",
    "        blotches = pd.read_csv(self.blotch_file)\n",
    "        combined = pd.concat([fans, blotches], sort=False)\n",
    "\n",
    "        obsids_with_data = combined.image_name.unique()\n",
    "        \n",
    "        if self.obsids.size == obsids_with_data.size:\n",
    "            \n",
    "            missing = list(set(self.obsids) - set(obsids_with_data))\n",
    "\n",
    "            LOGGER.warn(\"The following obsids have no data from clustering\")\n",
    "            LOGGER.warn(missing)\n",
    "\n",
    "        for obsid in tqdm(obsids_with_data):\n",
    "            data = combined[combined.image_name == obsid]\n",
    "            xy = XY2LATLON(data, self.savefolder, overwrite=self.overwrite)\n",
    "            xy.process_inpath()\n",
    "\n",
    "\n",
    "    def collect_marking_coordinates(self):\n",
    "        bucket = []\n",
    "        for obsid in self.obsids:\n",
    "            xy = XY2LATLON(None, self.savefolder, obsid=obsid)\n",
    "            bucket.append(pd.read_csv(xy.savepath).assign(obsid=obsid))\n",
    "\n",
    "        ground = pd.concat(bucket, sort=False).drop_duplicates()\n",
    "        ground.rename(dict(Sample=\"image_x\", Line=\"image_y\"), axis=1, inplace=True)\n",
    "        return ground\n",
    "\n",
    "    def fix_marking_coordinates_precision(self, df):\n",
    "        fname = \"tempfile.csv\"\n",
    "        df.to_csv(fname, float_format=\"%.7f\")\n",
    "        return pd.read_csv(fname, dtype=\"str\")\n",
    "\n",
    "    def merge_campt_results(self, fans, blotches):\n",
    "        INDEX = [\"obsid\", \"image_x\", \"image_y\"]\n",
    "\n",
    "        ground = self.collect_marking_coordinates().round(decimals=7)\n",
    "        # ground = self.fix_marking_coordinates_precision(ground)\n",
    "        fans = fans.merge(ground[self.COLS_TO_MERGE], on=INDEX)\n",
    "        blotches = blotches.merge(ground[self.COLS_TO_MERGE], on=INDEX)\n",
    "        return fans, blotches\n",
    "\n",
    "    def launch_catalog_production(self,kind : str = \"serial\", parallel_tasks : int = 10):\n",
    "\n",
    "        if kind == \"serial\":\n",
    "            self.launch_serial_production()\n",
    "        \n",
    "        if kind == \"parallel\":\n",
    "            self.launch_parallel_production(parallel_tasks=parallel_tasks)\n",
    "\n",
    "\n",
    "    def launch_parallel_production(self,parallel_tasks : int = 10):\n",
    "        \n",
    "        self.check_for_todo()\n",
    "        \n",
    "        fan_id = fan_id_generator()\n",
    "        blotch_id = blotch_id_generator()\n",
    "\n",
    "        #Simple trick to start too many tasks at the same time which all load a large DB.\n",
    "        total = len(self.todo)\n",
    "        #adding 1 to the loop amount is important to finish up the leftovers that dont fit in total/n_workers\n",
    "        # Example total = 10; n_workers = 3 => 10/3 = 3 meaning 3 loops until 0:3, 3:6, 6:9 , missing the last one 10\n",
    "        if total%parallel_tasks == 0:\n",
    "            loop_full = int(total/parallel_tasks)\n",
    "        else:\n",
    "            loop_full = int(np.floor(total/parallel_tasks)) + 1 \n",
    "        \n",
    "        for i in range(loop_full):\n",
    "            try: \n",
    "                temp_obsids = self.obsids[parallel_tasks*i:parallel_tasks*i+parallel_tasks]\n",
    "            except:\n",
    "                temp_obsids = self.obsids[parallel_tasks*i:]\n",
    "\n",
    "            LOGGER.info(f\"Performing the Clustering for batch {i}\")\n",
    "            _ = cluster_obsid_parallel(temp_obsids, self.catalog, self.dbname)\n",
    "\n",
    "            for obsid in temp_obsids:\n",
    "                paths = get_L1A_paths(obsid, self.catalog)\n",
    "                for path in paths:\n",
    "                    add_marking_ids(path, fan_id, blotch_id)\n",
    "            \n",
    "            # fnotch and apply cuts\n",
    "            LOGGER.info(\"Start fnotching\")\n",
    "            _ = fnotch_obsid_parallel(temp_obsids, self.catalog)\n",
    "\n",
    "            LOGGER.info(\"Creating the required RED45 mosaics for ground projections.\")\n",
    "            _ = execute_in_parallel(create_RED45_mosaic, temp_obsids)\n",
    "\n",
    "\n",
    "        # create summary CSV files of the clustering output\n",
    "        LOGGER.info(\"Creating L1C fan and blotch database files.\")\n",
    "        create_roi_file(self.obsids, self.catalog, self.catalog)\n",
    "\n",
    "        LOGGER.info(\"Calculating the center ground coordinates for all P4 tiles.\")\n",
    "        self.calc_tile_coordinates()\n",
    "\n",
    "        LOGGER.info(\"Calculating ground coordinates for catalog.\")\n",
    "        self.calc_marking_coordinates()\n",
    "\n",
    "        # calculate all metadata required for P4 analysis\n",
    "        LOGGER.info(\"Writing summary metadata file.\")\n",
    "        self.calc_metadata()\n",
    "        # merging metadata\n",
    "        self.merge_all()\n",
    "\n",
    "    \n",
    "    def launch_serial_production(self):\n",
    "        \"\"\"The Method for starting the production of the catalogue in a serial manner. Doing one OBSID at a time.\n",
    "        \"\"\"\n",
    "        self.check_for_todo()\n",
    "\n",
    "        fan_id = fan_id_generator()\n",
    "        blotch_id = blotch_id_generator()\n",
    "\n",
    "        for obsid in self.todo:\n",
    "\n",
    "            LOGGER.info(f\"Performing the Clustering for {obsid}\")\n",
    "            if len(self.todo) > 0:\n",
    "                cluster_obsid(obsid,self.catalog,dbname=self.dbname)\n",
    "\n",
    "                paths = get_L1A_paths(obsid, self.catalog)\n",
    "                for path in paths:\n",
    "                    add_marking_ids(path, fan_id, blotch_id)\n",
    "\n",
    "                LOGGER.info(f\"Start fnotching for {obsid}\")\n",
    "                fnotch_obsid(obsid,savedir=self.catalog)\n",
    "\n",
    "                create_RED45_mosaic(obsid)\n",
    "\n",
    "                self.mark_done(obsid)\n",
    "\n",
    "        LOGGER.info(\"Creating L1C fan and blotch database files.\")\n",
    "        create_roi_file(self.obsids, self.catalog, self.catalog)\n",
    "\n",
    "\n",
    "        \n",
    "        LOGGER.info(\"Calculating the center ground coordinates for all P4 tiles.\")\n",
    "        self.calc_tile_coordinates()\n",
    "\n",
    "        LOGGER.info(\"Calculating ground coordinates for catalog.\")\n",
    "        self.calc_marking_coordinates()\n",
    "\n",
    "        # calculate all metadata required for P4 analysis\n",
    "        LOGGER.info(\"Writing summary metadata file.\")\n",
    "        self.calc_metadata()\n",
    "        # merging metadata\n",
    "        self.merge_all()\n",
    "\n",
    "    def produce_single_obsid(self, obsid:str,  makeMosaics = True):\n",
    "        \"\"\"Clusters and creates all obsid data without merging \n",
    "           as this should only be done on the full catalog. This\n",
    "           is meant for repairing single obsids\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        obsid : str\n",
    "            One Singular obsid\n",
    "        makeMosaics : bool\n",
    "            wether you want to redownload and create the RED45 mosaics (not always necessary when rerunning)\n",
    "        \"\"\"\n",
    "\n",
    "        fan_id = fan_id_generator()\n",
    "        blotch_id = blotch_id_generator()\n",
    "\n",
    "        cluster_obsid(obsid,self.catalog,dbname=self.dbname)\n",
    "\n",
    "        paths = get_L1A_paths(obsid, self.catalog)\n",
    "        for path in paths:\n",
    "            add_marking_ids(path, fan_id, blotch_id)\n",
    "\n",
    "        LOGGER.info(f\"Start fnotching for {obsid}\")\n",
    "        fnotch_obsid(obsid,savedir=self.catalog)\n",
    "\n",
    "        if makeMosaics:\n",
    "            create_RED45_mosaic(obsid)\n",
    "        \n",
    "        self.mark_done(obsid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def read_csvfiles_into_lists_of_frames(folders):\n",
    "    \n",
    "    bucket = dict(fan=[], blotch=[])\n",
    "    for folder in folders:\n",
    "        for markingfile in folder.glob(\"*.csv\"):\n",
    "            key = \"fan\" if markingfile.name.endswith(\"fans.csv\") else \"blotch\"\n",
    "            bucket[key].append(pd.read_csv(markingfile))\n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>image_id</th>\n",
       "      <th>image_url</th>\n",
       "      <th>user_name</th>\n",
       "      <th>marking</th>\n",
       "      <th>x_tile</th>\n",
       "      <th>y_tile</th>\n",
       "      <th>acquisition_date</th>\n",
       "      <th>local_mars_time</th>\n",
       "      <th>...</th>\n",
       "      <th>image_y</th>\n",
       "      <th>radius_1</th>\n",
       "      <th>radius_2</th>\n",
       "      <th>distance</th>\n",
       "      <th>angle</th>\n",
       "      <th>spread</th>\n",
       "      <th>version</th>\n",
       "      <th>x_angle</th>\n",
       "      <th>y_angle</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50eefc1084af3b2f30000001</td>\n",
       "      <td>2013-01-10 17:36:16+00:00</td>\n",
       "      <td>APF0000sr1</td>\n",
       "      <td>http://www.planetfour.org/subjects/standard/50...</td>\n",
       "      <td>childinova</td>\n",
       "      <td>fan</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>2011-02-23</td>\n",
       "      <td>6:20 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>21143.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>288.600069</td>\n",
       "      <td>253.700089</td>\n",
       "      <td>6.136060</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.280665</td>\n",
       "      <td>-0.959806</td>\n",
       "      <td>ESP_021455_0935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50eefc1084af3b2f30000001</td>\n",
       "      <td>2013-01-10 17:36:16+00:00</td>\n",
       "      <td>APF0000sr1</td>\n",
       "      <td>http://www.planetfour.org/subjects/standard/50...</td>\n",
       "      <td>childinova</td>\n",
       "      <td>fan</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>2011-02-23</td>\n",
       "      <td>6:20 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>21088.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257.955423</td>\n",
       "      <td>259.953420</td>\n",
       "      <td>11.973597</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.174449</td>\n",
       "      <td>-0.984666</td>\n",
       "      <td>ESP_021455_0935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50eefc1084af3b2f30000001</td>\n",
       "      <td>2013-01-10 17:36:16+00:00</td>\n",
       "      <td>APF0000sr1</td>\n",
       "      <td>http://www.planetfour.org/subjects/standard/50...</td>\n",
       "      <td>childinova</td>\n",
       "      <td>fan</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>2011-02-23</td>\n",
       "      <td>6:20 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>20982.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>297.154842</td>\n",
       "      <td>67.231816</td>\n",
       "      <td>55.989031</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.387004</td>\n",
       "      <td>0.922078</td>\n",
       "      <td>ESP_021455_0935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50eeb87984af3b7266000001</td>\n",
       "      <td>2013-01-10 12:47:54+00:00</td>\n",
       "      <td>APF0000sq6</td>\n",
       "      <td>http://www.planetfour.org/subjects/standard/50...</td>\n",
       "      <td>bandnak</td>\n",
       "      <td>fan</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2011-02-23</td>\n",
       "      <td>6:20 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>5523.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>408.853274</td>\n",
       "      <td>82.127539</td>\n",
       "      <td>64.778311</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.136968</td>\n",
       "      <td>0.990575</td>\n",
       "      <td>ESP_021455_0935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50eeb87984af3b7266000001</td>\n",
       "      <td>2013-01-10 12:47:54+00:00</td>\n",
       "      <td>APF0000sq6</td>\n",
       "      <td>http://www.planetfour.org/subjects/standard/50...</td>\n",
       "      <td>bandnak</td>\n",
       "      <td>fan</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2011-02-23</td>\n",
       "      <td>6:20 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>5964.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>236.907155</td>\n",
       "      <td>60.405107</td>\n",
       "      <td>10.434250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.493864</td>\n",
       "      <td>0.869539</td>\n",
       "      <td>ESP_021455_0935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          classification_id                created_at    image_id  \\\n",
       "0  50eefc1084af3b2f30000001 2013-01-10 17:36:16+00:00  APF0000sr1   \n",
       "1  50eefc1084af3b2f30000001 2013-01-10 17:36:16+00:00  APF0000sr1   \n",
       "2  50eefc1084af3b2f30000001 2013-01-10 17:36:16+00:00  APF0000sr1   \n",
       "3  50eeb87984af3b7266000001 2013-01-10 12:47:54+00:00  APF0000sq6   \n",
       "4  50eeb87984af3b7266000001 2013-01-10 12:47:54+00:00  APF0000sq6   \n",
       "\n",
       "                                           image_url   user_name marking  \\\n",
       "0  http://www.planetfour.org/subjects/standard/50...  childinova     fan   \n",
       "1  http://www.planetfour.org/subjects/standard/50...  childinova     fan   \n",
       "2  http://www.planetfour.org/subjects/standard/50...  childinova     fan   \n",
       "3  http://www.planetfour.org/subjects/standard/50...     bandnak     fan   \n",
       "4  http://www.planetfour.org/subjects/standard/50...     bandnak     fan   \n",
       "\n",
       "   x_tile  y_tile acquisition_date local_mars_time  ...  image_y  radius_1  \\\n",
       "0       1      39       2011-02-23         6:20 PM  ...  21143.0       NaN   \n",
       "1       1      39       2011-02-23         6:20 PM  ...  21088.0       NaN   \n",
       "2       1      39       2011-02-23         6:20 PM  ...  20982.0       NaN   \n",
       "3       4      11       2011-02-23         6:20 PM  ...   5523.0       NaN   \n",
       "4       4      11       2011-02-23         6:20 PM  ...   5964.0       NaN   \n",
       "\n",
       "   radius_2    distance       angle     spread  version   x_angle   y_angle  \\\n",
       "0       NaN  288.600069  253.700089   6.136060      1.0 -0.280665 -0.959806   \n",
       "1       NaN  257.955423  259.953420  11.973597      1.0 -0.174449 -0.984666   \n",
       "2       NaN  297.154842   67.231816  55.989031      1.0  0.387004  0.922078   \n",
       "3       NaN  408.853274   82.127539  64.778311      1.0  0.136968  0.990575   \n",
       "4       NaN  236.907155   60.405107  10.434250      1.0  0.493864  0.869539   \n",
       "\n",
       "        image_name  \n",
       "0  ESP_021455_0935  \n",
       "1  ESP_021455_0935  \n",
       "2  ESP_021455_0935  \n",
       "3  ESP_021455_0935  \n",
       "4  ESP_021455_0935  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "newpath = Path(\"../../../../Data/Downselected.parq\")\n",
    "\n",
    "df = pd.read_parquet(newpath)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the Catalog from the Planet4 raw dataset we will use the pipeline in the following simple way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "newpath = Path(\"../../../../Data/Downselected.parq\")#This should point to the Dataset you wish to Catalogize\n",
    "db = io.DBManager(newpath)\n",
    "\n",
    "#Setting the Logger to Level 10 so we get all the INFO printed out\n",
    "LOGGER.setLevel(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ESP_021455_0935']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##The Relaese Manager is our main object that stores the important data like the path to the database\n",
    "##aswell as the name of the Folder to store the data\n",
    "rm = ReleaseManager(\"p4tools_test\", dbname=db.dbname, overwrite=False)\n",
    "\n",
    "#Check for which Images need to be finished and were not analysed yet.\n",
    "rm.check_for_todo()\n",
    "rm.todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should Log before\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd919d4ae284975b0750b967c148110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /home/tihro/Dropbox/masters_tom_ihro/Data/P4_catalog_p4tools_test/P4_catalog_p4tools_test_L1C_cut_0.5_fan.csv.\n",
      "Created /home/tihro/Dropbox/masters_tom_ihro/Data/P4_catalog_p4tools_test/P4_catalog_p4tools_test_L1C_cut_0.5_blotch.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 65.95it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling do_campt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.88s/it]\n",
      "/tmp/ipykernel_107328/2466343027.py:166: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  return all_data.groupby(\"image_name\").image_id.nunique()\n"
     ]
    }
   ],
   "source": [
    "#Finally Launch the production pipeline\n",
    "rm.launch_catalog_production()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planetary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
